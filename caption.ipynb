{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdRFFuNTc61I"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import time\n",
    "import cv2 \n",
    "import argparse\n",
    "import os \n",
    "import os.path as osp\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import random\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import pickle\n",
    "#import bcolz\n",
    "embedding_dim = 300\n",
    "max_caption_length = 25\n",
    "dim_image = 4096\n",
    "dim_hidden1 = 1000\n",
    "dim_hidden2 = 1000\n",
    "dim_input1 = 1000\n",
    "dim_input2 = 1300\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "yolo_weights_path = \"drive/My Drive/end_to_end/data/yolov3.weights\"#\"./data/yolov3.weights\"\n",
    "coco_names_path =  \"drive/My Drive/end_to_end/data/coco.names\"       #\"./data/coco.names\"\n",
    "yolo_config_path =  \"drive/My Drive/end_to_end/data/yolov3.cfg\"              #\"./data/yolov3.cfg\"\n",
    "embedding_path =  \"drive/My Drive/end_to_end/data/embedding.npy\"  #\"./data/embedding.npy\"\n",
    "wordtoindex_path =     \"drive/My Drive/end_to_end/data/wordtoindex.pickle\"                   #\"./data/wordtoindex.pickle\"\n",
    "indextoword_path =    \"drive/My Drive/end_to_end/data/indextoword.pickle\"                  #\"./data/indextoword.pickle\"\n",
    "video_path =    \"drive/My Drive/end_to_end/videos/dance.mp4\"    #\"./videos/0hyZ__3YhZc_485_490.avi\"\n",
    "model_save_path = \"/content/drive/My Drive/main/teacher_model\"\n",
    "model_num = 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6178,
     "status": "ok",
     "timestamp": 1564340719538,
     "user": {
      "displayName": "RUSHI JAYESHKUMAR BABARIYA",
      "photoUrl": "",
      "userId": "09653925580240235591"
     },
     "user_tz": -540
    },
    "id": "04roBGj7c61c",
    "outputId": "c14bfd5b-b502-4337-b43e-8b17a80acf28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embeddings and vocab.....\n"
     ]
    }
   ],
   "source": [
    "print(\"loading embeddings and vocab.....\")\n",
    "def save_dict(path , dct):\n",
    "    with open(path , \"wb\") as handle:\n",
    "        pickle.dump(dct , handle , protocol = pickle.HIGHEST_PROTOCOL)\n",
    "def load_dict(path):\n",
    "    with open(path, \"rb\") as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b\n",
    "\n",
    "embedding = torch.tensor(np.load(embedding_path)).to(device).double()\n",
    "\n",
    "wordtoindex = load_dict(wordtoindex_path)\n",
    "\n",
    "indextoword = load_dict(indextoword_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3CQ0iw_c61r"
   },
   "outputs": [],
   "source": [
    "def parse_cfg(cfgfile):\n",
    "    \"\"\"\n",
    "    Takes a configuration file\n",
    "    \n",
    "    Returns a list of blocks. Each blocks describes a block in the neural\n",
    "    network to be built. Block is represented as a dictionary in the list\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(cfgfile, 'r')\n",
    "    lines = file.read().split('\\n')                        # store the lines in a list\n",
    "    lines = [x for x in lines if len(x) > 0]               # get read of the empty lines \n",
    "    lines = [x for x in lines if x[0] != '#']              # get rid of comments\n",
    "    lines = [x.rstrip().lstrip() for x in lines]           # get rid of fringe whitespaces\n",
    "    \n",
    "    block = {}\n",
    "    blocks = []\n",
    "    \n",
    "    for line in lines:\n",
    "        if line[0] == \"[\":               # This marks the start of a new block\n",
    "            if len(block) != 0:          # If block is not empty, implies it is storing values of previous block.\n",
    "                blocks.append(block)     # add it the blocks list\n",
    "                block = {}               # re-init the block\n",
    "            block[\"type\"] = line[1:-1].rstrip()     \n",
    "        else:\n",
    "            key,value = line.split(\"=\") \n",
    "            block[key.rstrip()] = value.lstrip()\n",
    "    blocks.append(block)\n",
    "    \n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_COakjoc610"
   },
   "outputs": [],
   "source": [
    "def predict_transform(prediction, inp_dim, anchors, num_classes, CUDA = False):\n",
    "\n",
    "    \n",
    "    batch_size = prediction.size(0)\n",
    "    stride =  inp_dim // prediction.size(2)\n",
    "    grid_size = inp_dim // stride\n",
    "    bbox_attrs = 5 + num_classes\n",
    "    num_anchors = len(anchors)\n",
    "    \n",
    "    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n",
    "    prediction = prediction.transpose(1,2).contiguous()\n",
    "    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n",
    "    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n",
    "\n",
    "    #Sigmoid the  centre_X, centre_Y. and object confidencce\n",
    "    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n",
    "    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n",
    "    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n",
    "    \n",
    "    #Add the center offsets\n",
    "    grid = np.arange(grid_size)\n",
    "    a,b = np.meshgrid(grid, grid)\n",
    "\n",
    "    x_offset = torch.FloatTensor(a).view(-1,1)\n",
    "    y_offset = torch.FloatTensor(b).view(-1,1)\n",
    "\n",
    "    if CUDA:\n",
    "        x_offset = x_offset.cuda()\n",
    "        y_offset = y_offset.cuda()\n",
    "\n",
    "    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n",
    "\n",
    "    prediction[:,:,:2] += x_y_offset\n",
    "\n",
    "    #log space transform height and the width\n",
    "    anchors = torch.FloatTensor(anchors)\n",
    "\n",
    "    if CUDA:\n",
    "        anchors = anchors.cuda()\n",
    "\n",
    "    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n",
    "    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n",
    "    \n",
    "    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))\n",
    "\n",
    "    prediction[:,:,:4] *= stride\n",
    "    \n",
    "    return prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I9NTT98Nc61-"
   },
   "outputs": [],
   "source": [
    "class EmptyLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmptyLayer, self).__init__()\n",
    "        \n",
    "\n",
    "class DetectionLayer(nn.Module):\n",
    "    def __init__(self, anchors):\n",
    "        super(DetectionLayer, self).__init__()\n",
    "        self.anchors = anchors     \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0rT0qiwc62H"
   },
   "outputs": [],
   "source": [
    "def create_modules(blocks):\n",
    "    net_info = blocks[0]     #Captures the information about the input and pre-processing    \n",
    "    module_list = nn.ModuleList()\n",
    "    prev_filters = 3\n",
    "    output_filters = []\n",
    "    \n",
    "    for index, x in enumerate(blocks[1:]):\n",
    "        module = nn.Sequential()\n",
    "    \n",
    "        #check the type of block\n",
    "        #create a new module for the block\n",
    "        #append to module_list\n",
    "        \n",
    "        #If it's a convolutional layer\n",
    "        if (x[\"type\"] == \"convolutional\"):\n",
    "            #Get the info about the layer\n",
    "            activation = x[\"activation\"]\n",
    "            try:\n",
    "                batch_normalize = int(x[\"batch_normalize\"])\n",
    "                bias = False\n",
    "            except:\n",
    "                batch_normalize = 0\n",
    "                bias = True\n",
    "        \n",
    "            filters= int(x[\"filters\"])\n",
    "            padding = int(x[\"pad\"])\n",
    "            kernel_size = int(x[\"size\"])\n",
    "            stride = int(x[\"stride\"])\n",
    "        \n",
    "            if padding:\n",
    "                pad = (kernel_size - 1) // 2\n",
    "            else:\n",
    "                pad = 0\n",
    "        \n",
    "            #Add the convolutional layer\n",
    "            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)\n",
    "            module.add_module(\"conv_{0}\".format(index), conv)\n",
    "        \n",
    "            #Add the Batch Norm Layer\n",
    "            if batch_normalize:\n",
    "                bn = nn.BatchNorm2d(filters)\n",
    "                module.add_module(\"batch_norm_{0}\".format(index), bn)\n",
    "        \n",
    "            #Check the activation. \n",
    "            #It is either Linear or a Leaky ReLU for YOLO\n",
    "            if activation == \"leaky\":\n",
    "                activn = nn.LeakyReLU(0.1, inplace = True)\n",
    "                module.add_module(\"leaky_{0}\".format(index), activn)\n",
    "        \n",
    "            #If it's an upsampling layer\n",
    "            #We use Bilinear2dUpsampling\n",
    "        elif (x[\"type\"] == \"upsample\"):\n",
    "            stride = int(x[\"stride\"])\n",
    "            upsample = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n",
    "            module.add_module(\"upsample_{}\".format(index), upsample)\n",
    "                \n",
    "        #If it is a route layer\n",
    "        elif (x[\"type\"] == \"route\"):\n",
    "            x[\"layers\"] = x[\"layers\"].split(',')\n",
    "            #Start  of a route\n",
    "            start = int(x[\"layers\"][0])\n",
    "            #end, if there exists one.\n",
    "            try:\n",
    "                end = int(x[\"layers\"][1])\n",
    "            except:\n",
    "                end = 0\n",
    "            #Positive anotation\n",
    "            if start > 0: \n",
    "                start = start - index\n",
    "            if end > 0:\n",
    "                end = end - index\n",
    "            route = EmptyLayer()\n",
    "            module.add_module(\"route_{0}\".format(index), route)\n",
    "            if end < 0:\n",
    "                filters = output_filters[index + start] + output_filters[index + end]\n",
    "            else:\n",
    "                filters= output_filters[index + start]\n",
    "    \n",
    "        #shortcut corresponds to skip connection\n",
    "        elif x[\"type\"] == \"shortcut\":\n",
    "            shortcut = EmptyLayer()\n",
    "            module.add_module(\"shortcut_{}\".format(index), shortcut)\n",
    "            \n",
    "        #Yolo is the detection layer\n",
    "        elif x[\"type\"] == \"yolo\":\n",
    "            mask = x[\"mask\"].split(\",\")\n",
    "            mask = [int(x) for x in mask]\n",
    "    \n",
    "            anchors = x[\"anchors\"].split(\",\")\n",
    "            anchors = [int(a) for a in anchors]\n",
    "            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
    "            anchors = [anchors[i] for i in mask]\n",
    "    \n",
    "            detection = DetectionLayer(anchors)\n",
    "            module.add_module(\"Detection_{}\".format(index), detection)\n",
    "                              \n",
    "        module_list.append(module)\n",
    "        prev_filters = filters\n",
    "        output_filters.append(filters)\n",
    "    return (net_info, module_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlhSfAIZc62O"
   },
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "    def __init__(self, cfgfile):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.blocks = parse_cfg(cfgfile)\n",
    "        self.net_info, self.module_list = create_modules(self.blocks)\n",
    "        \n",
    "    def forward(self, x, CUDA):\n",
    "        modules = self.blocks[1:]\n",
    "        outputs = {}   #We cache the outputs for the route layer\n",
    "        \n",
    "        write = 0\n",
    "        for i, module in enumerate(modules):        \n",
    "            module_type = (module[\"type\"])\n",
    "            \n",
    "            if module_type == \"convolutional\" or module_type == \"upsample\":\n",
    "                x = self.module_list[i](x)\n",
    "    \n",
    "            elif module_type == \"route\":\n",
    "                layers = module[\"layers\"]\n",
    "                layers = [int(a) for a in layers]\n",
    "    \n",
    "                if (layers[0]) > 0:\n",
    "                    layers[0] = layers[0] - i\n",
    "    \n",
    "                if len(layers) == 1:\n",
    "                    x = outputs[i + (layers[0])]\n",
    "    \n",
    "                else:\n",
    "                    if (layers[1]) > 0:\n",
    "                        layers[1] = layers[1] - i\n",
    "    \n",
    "                    map1 = outputs[i + layers[0]]\n",
    "                    map2 = outputs[i + layers[1]]\n",
    "                    x = torch.cat((map1, map2), 1)\n",
    "                \n",
    "    \n",
    "            elif  module_type == \"shortcut\":\n",
    "                from_ = int(module[\"from\"])\n",
    "                x = outputs[i-1] + outputs[i+from_]\n",
    "    \n",
    "            elif module_type == 'yolo':        \n",
    "                anchors = self.module_list[i][0].anchors\n",
    "                #Get the input dimensionsthis video. Please check back late\n",
    "                inp_dim = int (self.net_info[\"height\"])\n",
    "        \n",
    "                #Get the number of classes\n",
    "                num_classes = int (module[\"classes\"])\n",
    "        \n",
    "                #Transform \n",
    "                x = x.data\n",
    "                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n",
    "                if not write:              #if no collector has been intialised. \n",
    "                    detections = x\n",
    "                    write = 1\n",
    "        \n",
    "                else:       \n",
    "                    detections = torch.cat((detections, x), 1)\n",
    "        \n",
    "            outputs[i] = x\n",
    "        \n",
    "        return detections\n",
    "    def load_weights(self , weightfile):\n",
    "        fp = open(weightfile , \"rb\")\n",
    "        header = np.fromfile(fp , dtype = np.int32 , count = 5)\n",
    "        self.header = torch.tensor(header)\n",
    "        self.seen = self.header[3]\n",
    "        weights = np.fromfile(fp , dtype = np.float32)\n",
    "        ptr = 0\n",
    "        for i in range(len(self.module_list)):\n",
    "            module_type = self.blocks[i + 1][\"type\"]\n",
    "            if(module_type == \"convolutional\"):\n",
    "                model = self.module_list[i]\n",
    "                try:\n",
    "                    batch_normalize = int(self.blocks[i + 1][\"batch_normalize\"])\n",
    "                except:\n",
    "                    batch_normalize = 0\n",
    "                conv = model[0]\n",
    "                if (batch_normalize):\n",
    "                    bn = model[1]\n",
    "\n",
    "                    #Get the number of weights of Batch Norm Layer\n",
    "                    num_bn_biases = bn.bias.numel()\n",
    "\n",
    "                    #Load the weights\n",
    "                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "\n",
    "                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "\n",
    "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "\n",
    "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "\n",
    "                    #Cast the loaded weights into dims of model weights. \n",
    "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
    "                    bn_weights = bn_weights.view_as(bn.weight.data)\n",
    "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
    "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
    "\n",
    "                    #Copy the data to model\n",
    "                    bn.bias.data.copy_(bn_biases)\n",
    "                    bn.weight.data.copy_(bn_weights)\n",
    "                    bn.running_mean.copy_(bn_running_mean)\n",
    "                    bn.running_var.copy_(bn_running_var)\n",
    "\n",
    "                else:\n",
    "                    num_biases = conv.bias.numel()\n",
    "\n",
    "                    #Load the weights\n",
    "                    conv_biases = torch.tensor(weights[ptr: ptr + num_biases])\n",
    "                    ptr = ptr + num_biases\n",
    "                    #reshape the loaded weights according to the dims of the model weights\n",
    "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
    "                    #Finally copy the data\n",
    "                    conv.bias.data.copy_(conv_biases)\n",
    "\n",
    "                #Let us load the weights for the Convolutional layers\n",
    "                num_weights = conv.weight.numel()\n",
    "\n",
    "                #Do the same as above for weights\n",
    "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
    "                ptr = ptr + num_weights\n",
    "\n",
    "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
    "                conv.weight.data.copy_(conv_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7288,
     "status": "ok",
     "timestamp": 1564340720759,
     "user": {
      "displayName": "RUSHI JAYESHKUMAR BABARIYA",
      "photoUrl": "",
      "userId": "09653925580240235591"
     },
     "user_tz": -540
    },
    "id": "RiYThODlc62W",
    "outputId": "6d7f35c6-87c1-4b1b-efc6-e106e7ac163c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading object detector...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading object detector...\")\n",
    "model = Darknet(yolo_config_path).to(device).eval()\n",
    "model.load_weights(yolo_weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gBDExI72c62g"
   },
   "outputs": [],
   "source": [
    "def unique(tensor):\n",
    "    tensor_np = tensor.cpu().numpy()\n",
    "    unique_np = np.unique(tensor_np)\n",
    "    unique_tensor = torch.tensor(unique_np)\n",
    "    tensor_res = tensor.new(unique_tensor.shape)\n",
    "    tensor_res.copy_(unique_tensor)\n",
    "    return tensor_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFn1B4Yfc62p"
   },
   "outputs": [],
   "source": [
    "def bbox_iou(box1 , box2):\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n",
    "    \n",
    "    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n",
    "    #Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n",
    "    \n",
    "    iou = inter_area / (b1_area + b2_area - inter_area)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBeAE_VYc62z"
   },
   "outputs": [],
   "source": [
    "def write_results(prediction , confidence , num_classes , nms_conf = .4):\n",
    "    conf_mask = (prediction[: , : , 4] > confidence).float().unsqueeze(2)\n",
    "    prediction = prediction*conf_mask\n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n",
    "    box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n",
    "    box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n",
    "    box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n",
    "    prediction[:,:,:4] = box_corner[:,:,:4]\n",
    "    batch_size = prediction.size(0)\n",
    "    write = False\n",
    "    for ind in range(batch_size):\n",
    "        image_pred = prediction[ind]\n",
    "        max_conf , max_conf_score  = torch.max(image_pred[: , 5:5 + num_classes] , 1)\n",
    "        max_conf = max_conf.float().unsqueeze(1)\n",
    "        max_conf_score = max_conf_score.float().unsqueeze(1)\n",
    "        seq = (image_pred[:,:5] , max_conf , max_conf_score)\n",
    "        image_pred = torch.cat(seq , 1)\n",
    "        non_zero_ind = (torch.nonzero(image_pred[:,4]))\n",
    "        try:\n",
    "            image_pred_ = image_pred[non_zero_ind.squeeze() , :].view(-1,7)\n",
    "        except:\n",
    "            continue\n",
    "        if(image_pred_.shape[0] == 0):\n",
    "            continue\n",
    "        img_classes = unique(image_pred_[:,-1])\n",
    "        for cls in img_classes:\n",
    "            cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n",
    "            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n",
    "            image_pred_class = image_pred_[class_mask_ind].view(-1, 7)\n",
    "            \n",
    "            conf_sort_index = torch.sort(image_pred_class[:,4] , descending = True)[1]\n",
    "            image_pred_class  =image_pred_class[conf_sort_index]\n",
    "            idx  =image_pred_class.size(0)\n",
    "            for i in range(idx):\n",
    "                try:\n",
    "                    ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n",
    "                except ValueError:\n",
    "                    break\n",
    "                except IndexError:\n",
    "                    break\n",
    "                iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
    "                image_pred_class[i+1:] = image_pred_class[i+1:]*iou_mask\n",
    "                non_zero_ind  = torch.nonzero(image_pred_class[: , 4])\n",
    "                image_pred_class = image_pred_class[non_zero_ind].view(-1,7)\n",
    "            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind) \n",
    "            seq = batch_ind, image_pred_class\n",
    "            if not write:\n",
    "                output = torch.cat(seq , 1)\n",
    "                write = True\n",
    "            else:\n",
    "                out = torch.cat(seq,1)\n",
    "                output  = torch.cat((output,out))\n",
    "                \n",
    "    try:\n",
    "        return output\n",
    "    except:\n",
    "        return 0\n",
    "                \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pzAAWyOOc628"
   },
   "outputs": [],
   "source": [
    "def load_classes(namesfile):\n",
    "    fp = open(namesfile, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7834,
     "status": "ok",
     "timestamp": 1564340721406,
     "user": {
      "displayName": "RUSHI JAYESHKUMAR BABARIYA",
      "photoUrl": "",
      "userId": "09653925580240235591"
     },
     "user_tz": -540
    },
    "id": "BVPxcFLBc63D",
    "outputId": "f8501f55-02a2-4306-ec91-d2390a3d926f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading object classes....\n"
     ]
    }
   ],
   "source": [
    "print(\"loading object classes....\")\n",
    "num_classes = 80    #For COCO\n",
    "classes = load_classes(coco_names_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11627,
     "status": "ok",
     "timestamp": 1564340725231,
     "user": {
      "displayName": "RUSHI JAYESHKUMAR BABARIYA",
      "photoUrl": "",
      "userId": "09653925580240235591"
     },
     "user_tz": -540
    },
    "id": "11GXImLfc63M",
    "outputId": "a7d07f8a-4e50-474e-e96e-5b7b2d49e801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading feature extractor...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading feature extractor...\")\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "vgg16.classifier = nn.Sequential(*[vgg16.classifier[i] for i in range(4)])\n",
    "for p in vgg16.parameters():\n",
    "    p.requires_grad = False\n",
    "vgg16 = vgg16.to(device).eval()\n",
    "num_frames = 80\n",
    "dim_embedding = 300    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGI99G3uc63V"
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(image, target_height=224, target_width=224):\n",
    "    if len(image.shape) == 2:\n",
    "        image = np.tile(image[:,:,None], 3)\n",
    "    elif len(image.shape) == 4:\n",
    "        image = image[:,:,:,0]\n",
    "\n",
    "    image = skimage.img_as_float(image).astype(np.float32)\n",
    "    height, width, rgb = image.shape\n",
    "    if width == height:\n",
    "        resized_image = cv2.resize(image, (target_height,target_width))\n",
    "\n",
    "    elif height < width:\n",
    "        resized_image = cv2.resize(image, (int(width * float(target_height)/height), target_width))\n",
    "        cropping_length = int((resized_image.shape[1] - target_height) / 2)\n",
    "        resized_image = resized_image[:,cropping_length:resized_image.shape[1] - cropping_length]\n",
    "\n",
    "    else:\n",
    "        resized_image = cv2.resize(image, (target_height, int(height * float(target_width) / width)))\n",
    "        cropping_length = int((resized_image.shape[0] - target_width) / 2)\n",
    "        resized_image = resized_image[cropping_length:resized_image.shape[0] - cropping_length,:]\n",
    "\n",
    "    return cv2.resize(resized_image, (target_height, target_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HV93_Gyic63b"
   },
   "outputs": [],
   "source": [
    "def convert_names(name):\n",
    "  if(name == 'traffic light'):\n",
    "    return \"traffic\"\n",
    "  elif(name == \"fire hydrant\"):\n",
    "    return \"extinguisher\"\n",
    "  elif(name == \"stop sign\"):\n",
    "    return \"signboard\"\n",
    "  elif(name == \"parking meter\"):\n",
    "    return \"meter\"\n",
    "  elif(name == \"sports ball\"):\n",
    "    return \"ball\"\n",
    "  elif(name == \"baseball bat\"):\n",
    "    return \"bat\"\n",
    "  elif(name == \"pottedplant\"):\n",
    "    return \"plant\"\n",
    "  elif(name == \"baseball glove\"):\n",
    "    return \"glove\"\n",
    "  elif(name == \"tennis racket\"):\n",
    "    return \"racket\"\n",
    "  elif(name == \"wine glass\"):\n",
    "    return \"glass\"\n",
    "  elif(name == \"hot dog\"):\n",
    "    return \"food\"\n",
    "  elif(name == \"diningtable\"):\n",
    "    return \"table\"\n",
    "  elif(name == \"tvmonitor\"):\n",
    "    return \"tv\"\n",
    "  elif(name ==  'cell phone'):\n",
    "    return \"phone\"\n",
    "  elif(name == \"teddy bear\"):\n",
    "    return \"toy\"\n",
    "  elif(name == \"hair drier\"):\n",
    "    return \"drier\"\n",
    "  elif(name == \"aeroplane\"):\n",
    "    return \"plane\" \n",
    "  else:\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11611,
     "status": "ok",
     "timestamp": 1564340725252,
     "user": {
      "displayName": "RUSHI JAYESHKUMAR BABARIYA",
      "photoUrl": "",
      "userId": "09653925580240235591"
     },
     "user_tz": -540
    },
    "id": "ANEzPz-wc63i",
    "outputId": "3da2c119-3528-4d90-a741-28ca1c1badb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading your video...(hope i enjoy it)\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading your video...(hope i enjoy it)\")\n",
    "cap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21242,
     "status": "ok",
     "timestamp": 1564340734896,
     "user": {
      "displayName": "RUSHI JAYESHKUMAR BABARIYA",
      "photoUrl": "",
      "userId": "09653925580240235591"
     },
     "user_tz": -540
    },
    "id": "J0snyZHPc63t",
    "outputId": "4679c3be-924e-4ccc-b8e0-a0af56f7c0fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaaaaawn .... That was a boring video :3\n"
     ]
    }
   ],
   "source": [
    "frame_count = 0\n",
    "frame_list = []\n",
    "\n",
    "while True:\n",
    "    ret , frame = cap.read()   \n",
    "    if(ret == False):\n",
    "        break\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_list.append(frame)\n",
    "    frame_count = frame_count + 1\n",
    "frame_list = np.array(frame_list)\n",
    "if frame_count > 80:\n",
    "    frame_indices = np.linspace(0, frame_count, num=num_frames, endpoint=False).astype(int)\n",
    "    frame_list = frame_list[frame_indices]\n",
    "cropped_array = []\n",
    "for i in range(frame_list.shape[0]):\n",
    "    cropped_array.append(preprocess_frame(frame_list[i]).transpose(2,0,1))\n",
    "vgg_tensor = torch.tensor(cropped_array).to(device)\n",
    "\n",
    "yolo_tensor  = torch.tensor(cropped_array).to(device)\n",
    "norm = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "for i in range(vgg_tensor.size(0)):\n",
    "    vgg_tensor[i] = norm(vgg_tensor[i])\n",
    "object_list = np.zeros((yolo_tensor.size(0) , dim_embedding))\n",
    "for yolo_frame in range(yolo_tensor.size(0)):\n",
    "    inp = yolo_tensor[yolo_frame:yolo_frame+1]\n",
    "    cuda = False\n",
    "    if(torch.cuda.is_available()):\n",
    "        cuda = True\n",
    "    pred = model(inp , cuda)\n",
    "    res = write_results(pred , .65 ,80 )\n",
    "      \n",
    "    if(type(res) is int):\n",
    "        pass\n",
    "    else:\n",
    "        _, index = torch.sort(res[: , 5] , 0 , descending = True)\n",
    "        res = res[index][0].to(\"cpu\").numpy()\n",
    "        class_index = int(res[-1].item())\n",
    "        obj  = classes[class_index]\n",
    "        object_list[yolo_frame] = embedding[wordtoindex[convert_names(obj)]].to(\"cpu\").numpy() \n",
    "vgg_features  = vgg16(vgg_tensor).detach().to(\"cpu\").numpy()\n",
    "print(\"yaaaaawn .... That was a boring video :3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21231,
     "status": "ok",
     "timestamp": 1564340734897,
     "user": {
      "displayName": "RUSHI JAYESHKUMAR BABARIYA",
      "photoUrl": "",
      "userId": "09653925580240235591"
     },
     "user_tz": -540
    },
    "id": "bY-HTMVNc631",
    "outputId": "4b8a2414-ede0-4cc0-f3f3-5ddadf5bbcc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading caption model\n"
     ]
    }
   ],
   "source": [
    "object_features = torch.tensor(object_list).to(device).double().unsqueeze(0)\n",
    "vgg_features = torch.tensor(vgg_features).to(device).double().unsqueeze(0)\n",
    "print(\"Loading caption model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rHuPR1ej57LR"
   },
   "outputs": [],
   "source": [
    "class Video_captioner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(wordtoindex), embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm1 = nn.LSTM(dim_input1 , dim_hidden1, batch_first = True)\n",
    "        self.lstm2 = nn.LSTM(dim_input2 , dim_hidden2 , batch_first = True)\n",
    "        self.img_encode = nn.Linear(dim_image , dim_input1)\n",
    "        self.embed_word = nn.Linear(dim_hidden2 ,len(wordtoindex))\n",
    "    def forward(self  , video , objects , feed , teacher = True ):\n",
    "        emb = self.embedding(feed)\n",
    "        out  = self.img_encode(video)\n",
    "        #encoding\n",
    "        out11 , state11 = self.lstm1(out)\n",
    "        lstm2_inp = torch.cat( (objects , out11) , dim = 2   )\n",
    "        out12 , state12 = self.lstm2(lstm2_inp)\n",
    "        #decoding\n",
    "        if(teacher == True):\n",
    "            padding = torch.zeros(video.size(0) , feed.size(1) , dim_input1).to(device).double()\n",
    "            out21 , state21 = self.lstm1(padding, state11)\n",
    "            lstm2_inp2 = torch.cat( (emb , out21)  , dim = 2 )\n",
    "            out22 , state22 = self.lstm2(lstm2_inp2 , state12)\n",
    "        else:\n",
    "            padding = torch.zeros(video.size(0) , max_caption_length-1 , dim_input1).to(device).double()\n",
    "            out21 , state21 = self.lstm1(padding , state11)\n",
    "            out22 = torch.zeros(video.size(0) , max_caption_length  -1, dim_hidden2).to(device).double()\n",
    "            state__ = state12\n",
    "            for i in range(max_caption_length -1):\n",
    "                inp_lstm2 = torch.cat((emb , out21[: , i:i+1, :]) , dim = 2)\n",
    "                out__ , state__ = self.lstm2(inp_lstm2 , state__)\n",
    "                out22[: , i:i+1 , :] = out__\n",
    "                with torch.no_grad():\n",
    "                    out__  = self.embed_word(out__)\n",
    "                    out__ = torch.argmax(out__ , dim = 2)                    \n",
    "                    out__ = self.embedding(out__)\n",
    "                    emb = out__\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "        out22 = out22.contiguous()    \n",
    "        soft  = self.embed_word(out22)\n",
    "        meaning_out = torch.matmul(soft , embedding)\n",
    "        return soft.view(-1, len(wordtoindex)) , meaning_out \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fgn1zS-SeUyh"
   },
   "outputs": [],
   "source": [
    "class Attention_model(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(wordtoindex), embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.img_encode = nn.Linear(dim_image , dim_input1)\n",
    "        self.lstm1 = nn.LSTM(dim_input1 , dim_hidden1, batch_first = True)\n",
    "        self.lstm2 = nn.LSTM(dim_input2 , dim_hidden2 , batch_first = True)\n",
    "        self.embed_word = nn.Linear(dim_hidden2 ,len(wordtoindex))\n",
    "        self.attn = nn.Linear(dim_hidden1  , dim_hidden2)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.context_inp = nn.Linear(dim_hidden1 , dim_hidden1)\n",
    "    def calculate_attention(self, prev_out , encoder_out):\n",
    "        seq_len = encoder_out.size(1)\n",
    "        encoder_out = self.attn(encoder_out)\n",
    "        encoder_out = encoder_out.transpose(1,2)\n",
    "        att_energy = torch.bmm(prev_out , encoder_out)\n",
    "        att_energy =  F.softmax(att_energy , dim = 2)\n",
    "        return att_energy\n",
    "    \n",
    "    def decoder_teacher(self , encoder_out , emb , decoder_state):\n",
    "        prev_out = torch.zeros(encoder_out.size(0) , 1 , dim_hidden2).to(device).double()\n",
    "        out22 = torch.zeros( encoder_out.size(0), max_caption_length -1 , dim_hidden2).to(device).double()\n",
    "        for i in range(max_caption_length - 1):\n",
    "            attention_energy = self.calculate_attention(prev_out , encoder_out)\n",
    "            context = torch.bmm(attention_energy , encoder_out)\n",
    "            context = self.context_inp(context)\n",
    "            lstm2_inp2 = torch.cat((emb[: , i:i+1 , :] , context ), dim = 2)\n",
    "            decoder_out , decoder_state = self.lstm2(lstm2_inp2 , decoder_state)\n",
    "            out22[: , i:i+1 , :] = decoder_out\n",
    "            prev_out = decoder_out\n",
    "        return out22\n",
    "    def decoder_non_teacher(self , encoder_out , emb , decoder_state):\n",
    "        prev_out = torch.zeros(encoder_out.size(0) , 1 , dim_hidden2).to(device).double()\n",
    "        out22 = torch.zeros(encoder_out.size(0) , max_caption_length -1 , dim_hidden2).to(device).double()\n",
    "        for i in range(max_caption_length - 1):\n",
    "            attention_energy = self.calculate_attention(prev_out , encoder_out)\n",
    "            context = torch.bmm(attention_energy , encoder_out)\n",
    "            context = self.context_inp(context)\n",
    "            lstm2_inp2 = torch.cat((emb , context ), dim = 2)\n",
    "            decoder_out , decoder_state = self.lstm2(lstm2_inp2 , decoder_state)\n",
    "            out22[: , i:i+1 , :] = decoder_out\n",
    "            prev_out = decoder_out\n",
    "            with torch.no_grad():\n",
    "                decoder_out  = self.embed_word(decoder_out)\n",
    "                decoder_out = torch.argmax(decoder_out , dim = 2)                    \n",
    "                emb = self.embedding(decoder_out)\n",
    "        return out22\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self  , video , objects , feed , teacher = True):\n",
    "        emb = self.embedding(feed)\n",
    "        video  = self.img_encode(video)\n",
    "        # encode\n",
    "        out11  , state11 = self.lstm1(video)\n",
    "        lstm2_inp = torch.cat( (objects , out11) , dim = 2 )\n",
    "        out12  , decoder_state  = self.lstm2(lstm2_inp)\n",
    "        encoder_out = out11\n",
    "        # Decode\n",
    "        if(teacher == True):\n",
    "            out22  = self.decoder_teacher(encoder_out , emb , decoder_state)\n",
    "        else:\n",
    "            out22 = self.decoder_non_teacher(encoder_out , emb , decoder_state)\n",
    "            \n",
    "        out22 = out22.contiguous() \n",
    "        soft  = self.embed_word(out22)\n",
    "        meaning_out = torch.matmul(soft , embedding)\n",
    "        return soft.view(-1, len(wordtoindex)) , meaning_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxHD75UwiEIJ"
   },
   "outputs": [],
   "source": [
    "video_model = Attention_model().to(device).double().eval()\n",
    "#video_model =  Video_captioner().to(device).double().eval()\n",
    "bos = torch.tensor(wordtoindex[\"<bos>\"]).to(device)\n",
    "bos = bos.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtPXSUuwjAYE"
   },
   "outputs": [],
   "source": [
    "def generate(model_num):\n",
    "    if(torch.cuda.is_available()):\n",
    "        video_model.load_state_dict(torch.load(os.path.join(model_save_path , \"model\" + str(model_num) + \".pt\")  ))\n",
    "    else:\n",
    "        video_model.load_state_dict(torch.load(os.path.join(model_save_path , \"model\" + str(model_num) + \".pt\") ,map_location='cpu'   ))\n",
    "    sent = []\n",
    "    video , yolo = vgg_features , object_features\n",
    "    out_ = video_model(video , yolo , bos , False)\n",
    "    out_ = torch.argmax(out_[0] , dim = 1).to(\"cpu\").numpy()\n",
    "    for i in out_:\n",
    "        w = indextoword[i]\n",
    "        if(w == \"<eos>\"):\n",
    "            break\n",
    "        sent.append(w)\n",
    "    sent = \" \".join(sent)\n",
    "\n",
    "    print(\" \")\n",
    "    print(\" \")\n",
    "    print(\">> \" , sent , \" <<\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23662,
     "status": "ok",
     "timestamp": 1564340737400,
     "user": {
      "displayName": "RUSHI JAYESHKUMAR BABARIYA",
      "photoUrl": "",
      "userId": "09653925580240235591"
     },
     "user_tz": -540
    },
    "id": "EQ4v716Sk_Rk",
    "outputId": "b8e0e809-2f5c-40be-aea5-1d857261f303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The caption is......drum rollllll\n",
      " \n",
      " \n",
      ">>  a woman is washing a box  <<\n"
     ]
    }
   ],
   "source": [
    "print(\"The caption is......drum rollllll\")\n",
    "generate(model_num)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "caption.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
