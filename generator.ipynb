{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generator.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"QwQkbHJ_GRfs","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import os\n","import tensorflow as tf\n","import pickle\n","import torch \n","import torch.nn as nn\n","from torch.nn.utils.rnn import pad_sequence\n","import random\n","import matplotlib.pyplot as plt\n","import re\n","import torch.nn.functional as F\n","vgg_path = \"drive/My Drive/common/cnn_features\"\n","corpus_path = \"drive/My Drive/common/video_corpus.csv\"\n","model_save_path1 = \"drive/My Drive/main/model_loss1\"\n","yolo_path = \"drive/My Drive/common/object_features\"\n","wordtoindex_path = \"drive/My Drive/common/wordtoindex.pickle\"\n","indextoword_path = \"drive/My Drive/common/indextoword.pickle\"\n","embedding_path = \"drive/My Drive/common/embedding.npy\"\n","train_names_path = \"drive/My Drive/common/train_names.npy\"\n","test_names_path = \"drive/My Drive/common/test_names.npy\"\n","valid_names_path = \"drive/My Drive/common/valid_names.npy\"\n","sentence_model_path = \"drive/My Drive/common/sentence_compare_model\"\n","model_save_path2 = \"drive/My Drive/main/model_loss2\"\n","model_save_path3 = \"drive/My Drive/main/model_loss3\"\n","model_save_prob = \"drive/My Drive/main/model_prob\"\n","model_save_teach = \"drive/My Drive/main/teacher_model\"\n","result_path = \"drive/My Drive/main/results_teach\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","max_caption_length = 25\n","dim_image = 4096\n","dim_hidden1 = 1000\n","dim_hidden2 = 1000\n","dim_input1 = 1000\n","dim_input2 = 1300\n","embedding_dim = 300\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e-oCpL7CGqpS","colab_type":"code","colab":{}},"source":["def save_dict(path , dct):\n","    with open(path , \"wb\") as handle:\n","        pickle.dump(dct , handle , protocol = pickle.HIGHEST_PROTOCOL)\n","def load_dict(path):\n","    with open(path, \"rb\") as handle:\n","        b = pickle.load(handle)\n","    return b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lVZnXwFZGjVM","colab_type":"code","colab":{}},"source":["embedding = np.load(embedding_path).astype(np.float32)\n","wordtoindex = load_dict(wordtoindex_path)\n","indextoword = load_dict(indextoword_path)\n","embedding = torch.tensor(embedding).to(device).double()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ye2pMbxHA2-","colab_type":"code","colab":{}},"source":["def clean(s):\n","    regex = re.compile('[^a-zA-Z\" \"]')\n","    s =  regex.sub(\"\" , s).strip().lower()\n","    new_w = [wordtoindex[\"<bos>\"]]\n","    for w in s.split(\" \"):\n","        if(w in wordtoindex):\n","            new_w.append(wordtoindex[w])\n","    new_w.append(wordtoindex[\"<eos>\"])\n","    return new_w\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A_aUX4zvGyVU","colab_type":"code","colab":{}},"source":["def get_data(tp):\n","    video_data = pd.read_csv(corpus_path, sep=',')\n","    video_data = video_data[video_data['Language'] == 'English']\n","    video_data['video_name'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n","    video_data['video_path'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n","    video_data['yolo_path'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n","    video_data['yolo_path'] = video_data['yolo_path'].map(lambda x: os.path.join(yolo_path, x))\n","    video_data['video_path'] = video_data['video_path'].map(lambda x: os.path.join(vgg_path, x))\n","    if(tp == \"train\"):\n","        names = np.load(train_names_path , allow_pickle = True)\n","\n","    elif(tp == \"test\"):\n","        names =  np.load(test_names_path , allow_pickle = True)\n","        video_data = video_data[video_data['video_name'].map(lambda x: x in names)]\n","        video_data = video_data[video_data['Description'].map(lambda x: isinstance(x, str))]\n","        unique_filenames = sorted(video_data['video_path'].unique())\n","        data = video_data[video_data['video_path'].map(lambda x: x in unique_filenames)]\n","        return data.groupby([\"video_path\" , \"yolo_path\"])\n","        \n","    elif(tp == \"valid\"):\n","        names = np.load(valid_names_path , allow_pickle = True)\n","    names = names.tolist()\n","    video_data = video_data[video_data['video_name'].map(lambda x: x in names)]\n","    video_data = video_data[video_data['Description'].map(lambda x: isinstance(x, str))]\n","    video_data[\"Description\"] = video_data[\"Description\"].map(lambda x : clean(x))\n","    video_data = video_data[video_data['Description'].map(lambda x: True if len(x) > 3 and len(x) < max_caption_length else False )]\n","    unique_filenames = sorted(video_data['video_path'].unique())\n","    data = video_data[video_data['video_path'].map(lambda x: x in unique_filenames)]\n","    print(len(data))\n","    return data.groupby([\"video_path\" , \"yolo_path\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X-Z6vWrRG5SL","colab_type":"code","colab":{}},"source":["df = get_data(\"test\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kbkv1A2mHHDW","colab_type":"code","colab":{}},"source":["class Video_captioner(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.embedding = nn.Embedding(len(wordtoindex), embedding_dim)\n","        self.embedding.weight = nn.Parameter(embedding)\n","        self.embedding.weight.requires_grad = False\n","        self.lstm1 = nn.LSTM(dim_input1 , dim_hidden1, batch_first = True)\n","        self.lstm2 = nn.LSTM(dim_input2 , dim_hidden2 , batch_first = True)\n","        self.img_encode = nn.Linear(dim_image , dim_input1)\n","        self.embed_word = nn.Linear(dim_hidden2 ,len(wordtoindex))\n","    def forward(self  , video , objects , feed , teacher = True ):\n","        emb = self.embedding(feed)\n","        out  = self.img_encode(video)\n","        #encoding\n","        out11 , state11 = self.lstm1(out)\n","        lstm2_inp = torch.cat( (objects , out11) , dim = 2   )\n","        out12 , state12 = self.lstm2(lstm2_inp)\n","        #decoding\n","        if(teacher == True):\n","            padding = torch.zeros(video.size(0) , feed.size(1) , dim_input1).to(device).double()\n","            out21 , state21 = self.lstm1(padding, state11)\n","            lstm2_inp2 = torch.cat( (emb , out21)  , dim = 2 )\n","            out22 , state22 = self.lstm2(lstm2_inp2 , state12)\n","        else:\n","            padding = torch.zeros(video.size(0) , max_caption_length-1 , dim_input1).to(device).double()\n","            out21 , state21 = self.lstm1(padding , state11)\n","            out22 = torch.zeros(video.size(0) , max_caption_length  -1, dim_hidden2).to(device).double()\n","            state__ = state12\n","            for i in range(max_caption_length -1):\n","                inp_lstm2 = torch.cat((emb , out21[: , i:i+1, :]) , dim = 2)\n","                out__ , state__ = self.lstm2(inp_lstm2 , state__)\n","                out22[: , i:i+1 , :] = out__\n","                with torch.no_grad():\n","                    out__  = self.embed_word(out__)\n","                    out__ = torch.argmax(out__ , dim = 2)                    \n","                    out__ = self.embedding(out__)\n","                    emb = out__\n","                    \n","                    \n","                \n","                \n","        out22 = out22.contiguous()    \n","        soft  = self.embed_word(out22)\n","        emb = torch.matmul(soft , embedding)\n","        return soft.view(-1, len(wordtoindex)) , emb \n","        \n","        \n","        \n","\n","        \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p-S9V3Nabins","colab_type":"code","colab":{}},"source":["class Attention_model(nn.Module):\n","    def __init__(self ):\n","        super().__init__()\n","        self.embedding = nn.Embedding(len(wordtoindex), embedding_dim)\n","        self.embedding.weight = nn.Parameter(embedding)\n","        self.embedding.weight.requires_grad = False\n","        self.img_encode = nn.Linear(dim_image , dim_input1)\n","        self.lstm1 = nn.LSTM(dim_input1 , dim_hidden1, batch_first = True)\n","        self.lstm2 = nn.LSTM(dim_input2 , dim_hidden2 , batch_first = True)\n","        self.embed_word = nn.Linear(dim_hidden2 ,len(wordtoindex))\n","        self.attn = nn.Linear(dim_hidden1  , dim_hidden2)\n","        self.tanh = nn.Tanh()\n","        self.context_inp = nn.Linear(dim_hidden1 , dim_hidden1)\n","    def calculate_attention(self, prev_out , encoder_out):\n","        seq_len = encoder_out.size(1)\n","        encoder_out = self.attn(encoder_out)\n","        encoder_out = encoder_out.transpose(1,2)\n","        att_energy = torch.bmm(prev_out , encoder_out)\n","        att_energy =  F.softmax(att_energy , dim = 2)\n","        return att_energy\n","    \n","    def decoder_teacher(self , encoder_out , emb , decoder_state):\n","        prev_out = torch.zeros(encoder_out.size(0) , 1 , dim_hidden2).to(device).double()\n","        out22 = torch.zeros( encoder_out.size(0), max_caption_length -1 , dim_hidden2).to(device).double()\n","        for i in range(max_caption_length - 1):\n","            attention_energy = self.calculate_attention(prev_out , encoder_out)\n","            context = torch.bmm(attention_energy , encoder_out)\n","            context = self.context_inp(context)\n","            lstm2_inp2 = torch.cat((emb[: , i:i+1 , :] , context ), dim = 2)\n","            decoder_out , decoder_state = self.lstm2(lstm2_inp2 , decoder_state)\n","            out22[: , i:i+1 , :] = decoder_out\n","            prev_out = decoder_out\n","        return out22\n","    def decoder_non_teacher(self , encoder_out , emb , decoder_state):\n","        prev_out = torch.zeros(encoder_out.size(0) , 1 , dim_hidden2).to(device).double()\n","        out22 = torch.zeros(encoder_out.size(0) , max_caption_length -1 , dim_hidden2).to(device).double()\n","        for i in range(max_caption_length - 1):\n","            attention_energy = self.calculate_attention(prev_out , encoder_out)\n","            context = torch.bmm(attention_energy , encoder_out)\n","            context = self.context_inp(context)\n","            lstm2_inp2 = torch.cat((emb , context ), dim = 2)\n","            decoder_out , decoder_state = self.lstm2(lstm2_inp2 , decoder_state)\n","            out22[: , i:i+1 , :] = decoder_out\n","            prev_out = decoder_out\n","            with torch.no_grad():\n","                decoder_out  = self.embed_word(decoder_out)\n","                decoder_out = torch.argmax(decoder_out , dim = 2)                    \n","                emb = self.embedding(decoder_out)\n","        return out22\n","        \n","            \n","        \n","        \n","\n","    def forward(self  , video , objects , feed , teacher = True):\n","        emb = self.embedding(feed)\n","        video  = self.img_encode(video)\n","        # encode\n","        out11  , state11 = self.lstm1(video)\n","        lstm2_inp = torch.cat( (objects , out11) , dim = 2 )\n","        out12  , decoder_state  = self.lstm2(lstm2_inp)\n","        encoder_out = out11\n","        # Decode\n","        if(teacher == True):\n","            out22  = self.decoder_teacher(encoder_out , emb , decoder_state)\n","        else:\n","            out22 = self.decoder_non_teacher(encoder_out , emb , decoder_state)\n","            \n","        out22 = out22.contiguous() \n","        soft  = self.embed_word(out22)\n","        meaning_out = torch.matmul(F.softmax(soft , dim = 2) , embedding)\n","        return soft.view(-1, len(wordtoindex)) , meaning_out "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g9LiPgIgG63a","colab_type":"code","colab":{}},"source":["#video_model = Video_captioner().to(device).double().eval()\n","video_model = Attention_model().to(device).double().eval()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kd-02kYcI1F6","colab_type":"code","colab":{}},"source":["bos = torch.tensor(wordtoindex[\"<bos>\"]).to(device)\n","bos = bos.unsqueeze(0).unsqueeze(0)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MJ8fuJxG7e2p","colab_type":"code","colab":{}},"source":["video_dict = {}\n","yolo_dict = {}\n","\n","for g in df:\n","    video_dict[g[0][0]] = torch.tensor(np.load(g[0][0] , allow_pickle = False))\n","    yolo_dict[g[0][1]] = torch.tensor(np.load(g[0][1] ,  allow_pickle = False))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6ZxDRyOIB-i","colab_type":"code","colab":{}},"source":["\n","def generate(model_num):\n","    video_model.load_state_dict(torch.load(os.path.join(model_save_teach , \"model\" + str(model_num) + \".pt\")  ))\n","    truth_dict_list = []\n","    gen_dict_list = []\n","    img_id = 0\n","    regex = re.compile('[^a-zA-Z\" \"]')\n","    for g in df:\n","        video = video_dict[g[0][0]].unsqueeze(0).double().to(device)\n","        yolo = yolo_dict[g[0][1]].unsqueeze(0).double().to(device)\n","        out_ = video_model(video , yolo , bos , False)\n","        out_ = torch.argmax(out_[0] , dim = 1).to(\"cpu\").numpy()\n","        sent = []\n","        for i in out_:\n","            w = indextoword[i]\n","            if(w == \"<eos>\"):\n","                break\n","            sent.append(w)\n","        sent = \" \".join(sent)\n","#        print(sent)\n","        gen_dict_list.append({'image_id': img_id, 'caption': sent})\n","        for j in g[1].Description.values:\n","            j =  regex.sub(\"\" , j).strip().lower()\n","#            print(j)\n","            truth_dict_list.append({'image_id': img_id, 'caption': j})\n","        img_id = img_id + 1 \n","#        print(\"\")\n","#        print(\"\")\n","    np.save(os.path.join(result_path , str(model_num) + \".npy\"  ) , [gen_dict_list , truth_dict_list , img_id])\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDgriQPvJ9gf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":469},"outputId":"026c38e1-be1a-4926-bdac-3d5afb835233"},"source":["for i in range(10,420,10):\n","    print(i)\n","    generate(i)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10\n","20\n","30\n","40\n","50\n","60\n","70\n","80\n","90\n","100\n","110\n","120\n","130\n","140\n","150\n","160\n","170\n","180\n","190\n","200\n","210\n","220\n","230\n","240\n","250\n","260\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lb2xhFHk9G60","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wo4IQbABxIgw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_lzjv7Iasmp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}