{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_train.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Ffs-LA8wbTgZ","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import os\n","import tensorflow as tf\n","import pickle\n","import torch \n","import torch.nn as nn\n","from torch.nn.utils.rnn import pad_sequence\n","import random\n","import matplotlib.pyplot as plt\n","import re\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","vgg_path = \"drive/My Drive/common/cnn_features\"\n","corpus_path = \"drive/My Drive/common/video_corpus.csv\"\n","model_save_path1 = \"drive/My Drive/main/model_loss1\"\n","yolo_path = \"drive/My Drive/common/object_features\"\n","wordtoindex_path = \"drive/My Drive/common/wordtoindex.pickle\"\n","indextoword_path = \"drive/My Drive/common/indextoword.pickle\"\n","embedding_path = \"drive/My Drive/common/embedding.npy\"\n","train_names_path = \"drive/My Drive/common/train_names.npy\"\n","test_names_path = \"drive/My Drive/common/test_names.npy\"\n","valid_names_path = \"drive/My Drive/common/valid_names.npy\"\n","sentence_model_path = \"drive/My Drive/common/new_sentence_model\"\n","model_save_path2 = \"drive/My Drive/main/model_loss2\"\n","model_save_path3 = \"drive/My Drive/main/model_loss3\"\n","model_save_prob = \"drive/My Drive/main/model_prob\"\n","model_save_teacher = \"drive/My Drive/main/teacher_model\"\n","model_sent_teacher = \"drive/My Drive/main/teacher_model_sent\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","sentence_model_number = 500\n","max_caption_length = 25\n","dim_image = 4096\n","dim_hidden1 = 1000\n","dim_hidden2 = 1000\n","dim_input1 = 1000\n","dim_input2 = 1300\n","embedding_dim = 300\n","batch_size = 50\n","lr = 0.00005\n","sent_lr = .00001\n","#video_model_num = 190\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rR6FKkxUecaI","colab_type":"code","colab":{}},"source":["def save_dict(path , dct):\n","    with open(path , \"wb\") as handle:\n","        pickle.dump(dct , handle , protocol = pickle.HIGHEST_PROTOCOL)\n","def load_dict(path):\n","    with open(path, \"rb\") as handle:\n","        b = pickle.load(handle)\n","    return b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A0KXkr81edyU","colab_type":"code","colab":{}},"source":["embedding = np.load(embedding_path).astype(np.float32)\n","wordtoindex = load_dict(wordtoindex_path)\n","indextoword = load_dict(indextoword_path)\n","embedding = torch.tensor(embedding).to(device).double()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1mjU4k8afHeL","colab_type":"code","colab":{}},"source":["def clean(s):\n","    regex = re.compile('[^a-zA-Z\" \"]')\n","    s =  regex.sub(\"\" , s).strip().lower()\n","    new_w = [wordtoindex[\"<bos>\"]]\n","    for w in s.split(\" \"):\n","        if(w in wordtoindex):\n","            new_w.append(wordtoindex[w])\n","    new_w.append(wordtoindex[\"<eos>\"])\n","    return new_w\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fwrhxx5-cSLi","colab_type":"code","colab":{}},"source":["def get_data(tp):\n","    video_data = pd.read_csv(corpus_path, sep=',')\n","    video_data = video_data[video_data['Language'] == 'English']\n","    video_data['video_name'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n","    video_data['video_path'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n","    video_data['yolo_path'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n","    video_data['yolo_path'] = video_data['yolo_path'].map(lambda x: os.path.join(yolo_path, x))\n","    video_data['video_path'] = video_data['video_path'].map(lambda x: os.path.join(vgg_path, x))\n","    if(tp == \"train\"):\n","        names = np.load(train_names_path , allow_pickle = True)\n","\n","    elif(tp == \"test\"):\n","        names =  np.load(test_names_path , allow_pickle = True)\n","    elif(tp == \"valid\"):\n","        names = np.load(valid_names_path , allow_pickle = True)\n","    names = names.tolist()\n","    video_data = video_data[video_data['video_name'].map(lambda x: x in names)]\n","    video_data = video_data[video_data['Description'].map(lambda x: isinstance(x, str))]\n","    video_data[\"Description\"] = video_data[\"Description\"].map(lambda x : clean(x))\n","    video_data = video_data[video_data['Description'].map(lambda x: True if len(x) > 3 and len(x) < max_caption_length else False )]\n","    unique_filenames = sorted(video_data['video_path'].unique())\n","    data = video_data[video_data['video_path'].map(lambda x: x in unique_filenames)]\n","    print(len(data))\n","    return data.groupby([\"video_path\" , \"yolo_path\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mD_NVH9kLOfD","colab_type":"code","colab":{}},"source":["def get_data_clean(tp):\n","    video_data = pd.read_csv(corpus_path, sep=',')\n","    video_data = video_data[video_data['Language'] == 'English']\n","    video_data = video_data[video_data['Source'] == 'clean']\n","    video_data['video_name'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n","    video_data['video_path'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n","    video_data['yolo_path'] = video_data.apply(lambda row: row['VideoID']+'_'+str(int(row['Start']))+'_'+str(int(row['End']))+'.avi.npy', axis=1)\n","    video_data['yolo_path'] = video_data['yolo_path'].map(lambda x: os.path.join(yolo_path, x))\n","    video_data['video_path'] = video_data['video_path'].map(lambda x: os.path.join(vgg_path, x))\n","    if(tp == \"train\"):\n","        names = np.load(train_names_path , allow_pickle = True)\n","\n","    elif(tp == \"test\"):\n","        names =  np.load(test_names_path , allow_pickle = True)\n","    elif(tp == \"valid\"):\n","        names = np.load(valid_names_path , allow_pickle = True)\n","    names = names.tolist()\n","    video_data = video_data[video_data['video_name'].map(lambda x: x in names)]\n","    video_data = video_data[video_data['Description'].map(lambda x: isinstance(x, str))]\n","    video_data[\"Description\"] = video_data[\"Description\"].map(lambda x : clean(x))\n","    video_data = video_data[video_data['Description'].map(lambda x: True if len(x) > 3 and len(x) < max_caption_length else False )]\n","    unique_filenames = sorted(video_data['video_path'].unique())\n","    data = video_data[video_data['video_path'].map(lambda x: x in unique_filenames)]\n","    print(len(data))\n","    return data.groupby([\"video_path\" , \"yolo_path\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NvG8TTjyl6Iv","colab_type":"code","outputId":"01d8df9e-a104-41b2-f606-693c157d1718","executionInfo":{"status":"ok","timestamp":1563862285034,"user_tz":-540,"elapsed":29358,"user":{"displayName":"RUSHI JAYESHKUMAR BABARIYA","photoUrl":"","userId":"09653925580240235591"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["df = get_data(\"train\")\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["49175\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xiIYDLWvrJuK","colab_type":"code","colab":{}},"source":["video_dict = {}\n","yolo_dict = {}\n","\n","for g in df:\n","    video_dict[g[0][0]] = torch.tensor(np.load(g[0][0] , allow_pickle = False))\n","    yolo_dict[g[0][1]] = torch.tensor(np.load(g[0][1] ,  allow_pickle = False))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGBBNWthd34x","colab_type":"code","outputId":"ae79cd87-5f3f-4560-df1b-1db8055bd9bc","executionInfo":{"status":"ok","timestamp":1563862300363,"user_tz":-540,"elapsed":44626,"user":{"displayName":"RUSHI JAYESHKUMAR BABARIYA","photoUrl":"","userId":"09653925580240235591"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["gru_inp = 1000\n","gru_hidden = 1000\n","gru_inp = 1000\n","gru_hidden = 1000\n","class sim(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.emb_gru = nn.Linear(embedding_dim , gru_hidden)\n","        self.gru = nn.GRU(gru_inp,  gru_hidden  , batch_first = True,bidirectional = True)\n","        self.embedding = nn.Embedding(len(wordtoindex), embedding_dim)\n","        self.embedding.weight = nn.Parameter(embedding)\n","        self.embedding.weight.requires_grad = False\n","        self.gru_last = nn.Linear(gru_hidden*2 , gru_hidden)\n","         \n","    def forward(self , X):\n","        emb = self.embedding(X)\n","        out = self.emb_gru(emb)\n","        out , _ = self.gru(out)\n","        out = out[: , -1, :]\n","        out = self.gru_last(out)\n","        return out\n","        \n","\n","model = sim().to(device).double()\n","model_num = sentence_model_number\n","model.load_state_dict(torch.load(os.path.join(sentence_model_path , \"model\" + str(model_num) + \".pt\") ))\n","#model  = model.eval()\n"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["IncompatibleKeys(missing_keys=[], unexpected_keys=[])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"T2e0tQv5laEQ","colab_type":"code","colab":{}},"source":["def get_batch(df):\n","    batch = df.apply(lambda x : x.iloc[np.random.choice(len(x))])\n","    batch = batch.reset_index(drop = True)\n","    batch = batch.sample(batch_size)\n","    videos = list(map(lambda x : video_dict[x],   batch[\"video_path\"].values))\n","    objects = list(map(lambda x :yolo_dict[x] ,   batch[\"yolo_path\"].values))\n","    videos = pad_sequence(videos , batch_first = True)\n","    objects = pad_sequence(objects , batch_first = True)\n","    description = batch.Description.tolist()\n","    description = list(map(lambda x : torch.tensor(x) , description))\n","    \n","    description = pad_sequence(description , batch_first = True)\n","    pad = torch.zeros(description.size(0) , max_caption_length - description.size(1)).long()\n","    description = torch.cat((description , pad) , dim = 1)\n","    targets = description[: , 1:]\n","    feed  = description[: , :-1]\n","    \n","#    for v in videos:\n","#        print(v.shape)\n","#        break\n","    return videos.to(device).double() , objects.to(device).double() , feed.to(device)  ,  targets.to(device) \n","    \n","\n","\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8SpRRw32lEAQ","colab_type":"code","colab":{}},"source":["dist = nn.PairwiseDistance(p=1 , eps = 1e-6)\n","def distance(dist_model, emb , batch_size):\n","    out = dist_model.emb_gru(emb)\n","    out , _ = dist_model.gru(out)\n","    out = out[: , -1, :]\n","    out = dist_model.gru_last(out)\n","    s1  = out[:batch_size]\n","    s2  = out[batch_size:]\n","    l = s1.size(0)//2\n","    s3  = torch.cat((s1[:l] , s2[:l]) , dim = 0)\n","    s4  = torch.cat((s1[l:] , s2[l:]) , dim = 0)\n","    return dist(s1,s2) , dist(s3,s4)\n","    \n","    \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x3m6im8YfRA7","colab_type":"code","colab":{}},"source":["\n","class Video_captioner(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.embedding = nn.Embedding(len(wordtoindex), embedding_dim)\n","        self.embedding.weight = nn.Parameter(embedding)\n","        self.embedding.weight.requires_grad = False\n","        self.lstm1 = nn.LSTM(dim_input1 , dim_hidden1, batch_first = True)\n","        self.lstm2 = nn.LSTM(dim_input2 , dim_hidden2 , batch_first = True)\n","        self.img_encode = nn.Linear(dim_image , dim_input1)\n","        self.embed_word = nn.Linear(dim_hidden2 ,len(wordtoindex))\n","    def forward(self  , video , objects , feed , teacher = True ):\n","        emb = self.embedding(feed)\n","        out  = self.img_encode(video)\n","        #encoding\n","        out11 , state11 = self.lstm1(out)\n","        lstm2_inp = torch.cat( (objects , out11) , dim = 2   )\n","        out12 , state12 = self.lstm2(lstm2_inp)\n","        #decoding\n","        if(teacher == True):\n","            padding = torch.zeros(video.size(0) , feed.size(1) , dim_input1).to(device).double()\n","            out21 , state21 = self.lstm1(padding, state11)\n","            lstm2_inp2 = torch.cat( (emb , out21)  , dim = 2 )\n","            out22 , state22 = self.lstm2(lstm2_inp2 , state12)\n","        else:\n","            padding = torch.zeros(video.size(0) , max_caption_length-1 , dim_input1).to(device).double()\n","            out21 , state21 = self.lstm1(padding , state11)\n","            out22 = torch.zeros(video.size(0) , max_caption_length  -1, dim_hidden2).to(device).double()\n","            state__ = state12\n","            for i in range(max_caption_length -1):\n","                inp_lstm2 = torch.cat((emb , out21[: , i:i+1, :]) , dim = 2)\n","                out__ , state__ = self.lstm2(inp_lstm2 , state__)\n","                out22[: , i:i+1 , :] = out__\n","                with torch.no_grad():\n","                    out__  = self.embed_word(out__)\n","                    out__ = torch.argmax(out__ , dim = 2)                    \n","                    out__ = self.embedding(out__)\n","                    emb = out__\n","                    \n","                    \n","                \n","                \n","        out22 = out22.contiguous()    \n","        soft  = self.embed_word(out22)\n","        meaning_out = torch.matmul(soft , embedding)\n","        return soft.view(-1, len(wordtoindex)) , meaning_out \n","        \n","        \n","        \n","\n","        \n","        \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lhx8dzLPisU-","colab_type":"code","colab":{}},"source":["\n","class Attention_model(nn.Module):\n","    def __init__(self ):\n","        super().__init__()\n","        self.embedding = nn.Embedding(len(wordtoindex), embedding_dim)\n","        self.embedding.weight = nn.Parameter(embedding)\n","        self.embedding.weight.requires_grad = False\n","        self.img_encode = nn.Linear(dim_image , dim_input1)\n","        self.lstm1 = nn.LSTM(dim_input1 , dim_hidden1, batch_first = True)\n","        self.lstm2 = nn.LSTM(dim_input2 , dim_hidden2 , batch_first = True)\n","        self.embed_word = nn.Linear(dim_hidden2 ,len(wordtoindex))\n","        self.attn = nn.Linear(dim_hidden1  , dim_hidden2)\n","        self.tanh = nn.Tanh()\n","        self.context_inp = nn.Linear(dim_hidden1 , dim_hidden1)\n","    def calculate_attention(self, prev_out , encoder_out):\n","        seq_len = encoder_out.size(1)\n","        encoder_out = self.attn(encoder_out)\n","        encoder_out = encoder_out.transpose(1,2)\n","        att_energy = torch.bmm(prev_out , encoder_out)\n","        att_energy =  F.softmax(att_energy , dim = 2)\n","        return att_energy\n","    \n","    def decoder_teacher(self , encoder_out , emb , decoder_state):\n","        prev_out = torch.zeros(encoder_out.size(0) , 1 , dim_hidden2).to(device).double()\n","        out22 = torch.zeros( encoder_out.size(0), max_caption_length -1 , dim_hidden2).to(device).double()\n","        for i in range(max_caption_length - 1):\n","            attention_energy = self.calculate_attention(prev_out , encoder_out)\n","            context = torch.bmm(attention_energy , encoder_out)\n","            context = self.context_inp(context)\n","            lstm2_inp2 = torch.cat((emb[: , i:i+1 , :] , context ), dim = 2)\n","            decoder_out , decoder_state = self.lstm2(lstm2_inp2 , decoder_state)\n","            out22[: , i:i+1 , :] = decoder_out\n","            prev_out = decoder_out\n","        return out22\n","    def decoder_non_teacher(self , encoder_out , emb , decoder_state):\n","        prev_out = torch.zeros(encoder_out.size(0) , 1 , dim_hidden2).to(device).double()\n","        out22 = torch.zeros(encoder_out.size(0) , max_caption_length -1 , dim_hidden2).to(device).double()\n","        for i in range(max_caption_length - 1):\n","            attention_energy = self.calculate_attention(prev_out , encoder_out)\n","            context = torch.bmm(attention_energy , encoder_out)\n","            context = self.context_inp(context)\n","            lstm2_inp2 = torch.cat((emb , context ), dim = 2)\n","            decoder_out , decoder_state = self.lstm2(lstm2_inp2 , decoder_state)\n","            out22[: , i:i+1 , :] = decoder_out\n","            prev_out = decoder_out\n","            with torch.no_grad():\n","                decoder_out  = self.embed_word(decoder_out)\n","                decoder_out = torch.argmax(decoder_out , dim = 2)                    \n","                emb = self.embedding(decoder_out)\n","        return out22\n","        \n","            \n","        \n","        \n","\n","    def forward(self  , video , objects , feed , teacher = True):\n","        emb = self.embedding(feed)\n","        video  = self.img_encode(video)\n","        # encode\n","        out11  , state11 = self.lstm1(video)\n","        lstm2_inp = torch.cat( (objects , out11) , dim = 2 )\n","        out12  , decoder_state  = self.lstm2(lstm2_inp)\n","        encoder_out = out11\n","        # Decode\n","        if(teacher == True):\n","            out22  = self.decoder_teacher(encoder_out , emb , decoder_state)\n","        else:\n","            out22 = self.decoder_non_teacher(encoder_out , emb , decoder_state)\n","            \n","        out22 = out22.contiguous() \n","        soft  = self.embed_word(out22)\n","        meaning_out = torch.matmul(F.softmax(soft , dim = 2) , embedding)\n","        return soft.view(-1, len(wordtoindex)) , meaning_out "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fVJ-T5jvgMRV","colab_type":"code","colab":{}},"source":["#video_model = Video_captioner().to(device).double()\n","video_model = Attention_model().to(device).double()\n","video_model.load_state_dict(torch.load(os.path.join(model_save_path3 , \"model\" + str(370) + \".pt\")))\n","#video_model.lstm1.requires_grad = False\n","#video_model.lstm2.requires_grad = False\n","#video_model.attn.requires_grad = False\n","params_video = list(filter(lambda p: p.requires_grad, video_model.parameters()))\n","params_sent =  list(filter(lambda p: p.requires_grad, model.parameters()))\n","optimizer = torch.optim.Adam(params_video, lr)\n","sent_optimizer = torch.optim.Adam(params_sent, sent_lr)\n","crit_video = torch.nn.CrossEntropyLoss()\n","epoochs = 500"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJIJ6Goy1CXL","colab_type":"code","colab":{}},"source":["def loss1(out , targets):\n","    targets = targets.contiguous().view(-1)\n","    return crit_video(out , targets)\n","\n","def loss2(emb1 , targets):\n","    emb2  = video_model.embedding(targets)\n","    emb = torch.cat((emb1 , emb2) , dim = 0)\n","    dis = distance(model ,emb , emb1.size(0))\n","    loss_same = torch.exp(-dis[0]).mean()\n","    loss_diff = torch.exp(-dis[1]).mean()\n","    return (1 - loss_same) , loss_diff\n","\n","total_loss_arr   = []\n","meaning_array = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vTwi8BEaFKDG","colab_type":"code","colab":{}},"source":["bos = torch.tensor(wordtoindex[\"<bos>\"]).to(device)\n","bos = bos.unsqueeze(0).unsqueeze(0)\n","bos = bos.expand(batch_size , 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L5J4TR_7GCCQ","colab_type":"code","outputId":"e974b38c-ba3c-41e4-8654-2a50056c107c","executionInfo":{"status":"ok","timestamp":1563862304406,"user_tz":-540,"elapsed":48456,"user":{"displayName":"RUSHI JAYESHKUMAR BABARIYA","photoUrl":"","userId":"09653925580240235591"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["bos.shape"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([50, 1])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"OBB5NG9zgeAx","colab_type":"code","colab":{}},"source":["\n","def train1(d):\n","    out = video_model(d[0] , d[1] , d[2] , True)\n","    l1 = loss1(out[0] , d[3])\n","    loss = l1 \n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    return loss.data.item()\n","def train3(d):\n","    out = video_model(d[0] , d[1] , bos , False)\n","    l1 = loss1(out[0] , d[3])\n","    loss = l1 \n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    return loss.data.item()\n","\n","        \n","            "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wsg7KJhG6zav","colab_type":"code","colab":{}},"source":["def train2(d):\n","    out = video_model(d[0] , d[1] , bos , False)\n","    l2_same , l2_diff = loss2(out[1] , d[3])\n","    l2_same.backward( retain_graph=True)\n","    sent_optimizer.step()\n","    sent_optimizer.zero_grad()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","    l2_diff.backward()\n","    sent_optimizer.step()\n","    sent_optimizer.zero_grad()\n","    loss  = l2_same + l2_diff\n","    return loss.data.item()\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1WkFmJPKXFl","colab_type":"code","colab":{}},"source":["rep = 1200//batch_size\n","def train():\n","    for epoch in range(epoochs):\n","        print(epoch)\n","        total_loss = 0\n","        total_meaning = 0\n","        count1 = 0\n","        count2 = 0\n","        for _ in range(rep):\n","            d = get_batch(df)\n","            flag = random.random()\n","            if(flag <= .35 ):\n","                l1 =  train1(d)\n","                total_loss = total_loss + l1\n","                count1 = count1 + 1\n","            else:\n","                l2 = train2(d) \n","                total_meaning = total_meaning + l2\n","                count2 = count2 + 1\n","        if(count1 != 0):\n","            total_loss = total_loss/count1\n","        if(count2 != 0):\n","            total_meaning = total_meaning/count2\n","        total_loss_arr.append(total_loss)\n","        meaning_array.append(total_meaning)\n","        print(\"teacher loss  count\" , total_loss , count1)\n","        print(\"non teach loss count\", total_meaning , count2)\n","        if((epoch + 1)%10 == 0):\n","            torch.save(model.state_dict(), os.path.join(model_sent_teacher , \"model\" + str(epoch + 1) + \".pt\"))\n","            torch.save(video_model.state_dict(), os.path.join(model_save_teacher , \"model\" + str(epoch + 1) + \".pt\"))\n","            print(\"model saved\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"giw68KTY25QJ","colab_type":"code","outputId":"d3f2ad47-379f-46a3-e496-558d1676bc56","executionInfo":{"status":"ok","timestamp":1563433288829,"user_tz":-540,"elapsed":4748261,"user":{"displayName":"RUSHI JAYESHKUMAR BABARIYA","photoUrl":"","userId":"09653925580240235591"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["train()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","teacher loss  count 0.7110329452850973 12\n","non teach loss count 0.38573355422597705 12\n","1\n","teacher loss  count 0.7171902356825006 8\n","non teach loss count 0.3078689324031274 16\n","2\n","teacher loss  count 0.6535796231250711 7\n","non teach loss count 0.28292197023754817 17\n","3\n","teacher loss  count 0.6595703306457105 8\n","non teach loss count 0.2597542042192105 16\n","4\n","teacher loss  count 0.695228827657072 10\n","non teach loss count 0.24125750209489696 14\n","5\n","teacher loss  count 0.7734968959252618 1\n","non teach loss count 0.2367421629152599 23\n","6\n","teacher loss  count 0.6027518915871376 6\n","non teach loss count 0.22778151021664395 18\n","7\n","teacher loss  count 0.7290023228706628 5\n","non teach loss count 0.23034795523893178 19\n","8\n","teacher loss  count 0.7742158298067789 9\n","non teach loss count 0.19821918345050482 15\n","9\n","teacher loss  count 0.6683137946318228 5\n","non teach loss count 0.19564014038210664 19\n","model saved\n","10\n","teacher loss  count 0.7341490331192466 5\n","non teach loss count 0.19727749307646003 19\n","11\n","teacher loss  count 0.6864289939545772 8\n","non teach loss count 0.1862135487971884 16\n","12\n","teacher loss  count 0.7473680810660438 7\n","non teach loss count 0.19251233541433704 17\n","13\n","teacher loss  count 0.7165904455422665 8\n","non teach loss count 0.19377405752575444 16\n","14\n","teacher loss  count 0.6971750573295363 7\n","non teach loss count 0.18924660795198142 17\n","15\n","teacher loss  count 0.6901740167117435 7\n","non teach loss count 0.17538583069246316 17\n","16\n","teacher loss  count 0.6893335250865236 9\n","non teach loss count 0.1745010220764283 15\n","17\n","teacher loss  count 0.7704414532374356 9\n","non teach loss count 0.17833380013864442 15\n","18\n","teacher loss  count 0.7026757719204882 6\n","non teach loss count 0.18539826346829494 18\n","19\n","teacher loss  count 0.6806938388575168 8\n","non teach loss count 0.1901036841321934 16\n","model saved\n","20\n","teacher loss  count 0.7040858293233041 5\n","non teach loss count 0.16173961445540844 19\n","21\n","teacher loss  count 0.727781652091437 12\n","non teach loss count 0.1665020764838372 12\n","22\n","teacher loss  count 0.7441082516471099 12\n","non teach loss count 0.17232927729839867 12\n","23\n","teacher loss  count 0.683067824902127 10\n","non teach loss count 0.17629459134295364 14\n","24\n","teacher loss  count 0.7053044829656425 11\n","non teach loss count 0.15400484330321168 13\n","25\n","teacher loss  count 0.7179764173923606 8\n","non teach loss count 0.17379659498990122 16\n","26\n","teacher loss  count 0.7057532522162381 9\n","non teach loss count 0.1611839984705794 15\n","27\n","teacher loss  count 0.6669803578706053 10\n","non teach loss count 0.17514663155350768 14\n","28\n","teacher loss  count 0.6939576500990338 10\n","non teach loss count 0.17300351076886314 14\n","29\n","teacher loss  count 0.7153385027459082 10\n","non teach loss count 0.16638142937364145 14\n","model saved\n","30\n","teacher loss  count 0.7211817488484392 6\n","non teach loss count 0.1602840577319354 18\n","31\n","teacher loss  count 0.688554144453977 11\n","non teach loss count 0.1605897530544744 13\n","32\n","teacher loss  count 0.6724018294017959 7\n","non teach loss count 0.16121817450944717 17\n","33\n","teacher loss  count 0.6891864397461461 8\n","non teach loss count 0.17346908908152944 16\n","34\n","teacher loss  count 0.6946413059763212 11\n","non teach loss count 0.159747595033403 13\n","35\n","teacher loss  count 0.6938409500647992 12\n","non teach loss count 0.16967168347854208 12\n","36\n","teacher loss  count 0.7595743912574284 6\n","non teach loss count 0.1715084255837461 18\n","37\n","teacher loss  count 0.6904789518210528 8\n","non teach loss count 0.16632523258488646 16\n","38\n","teacher loss  count 0.7000570035029775 11\n","non teach loss count 0.17580893623545452 13\n","39\n","teacher loss  count 0.6808090452008652 6\n","non teach loss count 0.15204677760940052 18\n","model saved\n","40\n","teacher loss  count 0.6811439268289343 9\n","non teach loss count 0.16169170069877878 15\n","41\n","teacher loss  count 0.6847895765321291 9\n","non teach loss count 0.15064462915416976 15\n","42\n","teacher loss  count 0.6674891630212515 6\n","non teach loss count 0.1595457611132242 18\n","43\n","teacher loss  count 0.6920182100537913 10\n","non teach loss count 0.14184932493176047 14\n","44\n","teacher loss  count 0.7123618277047059 8\n","non teach loss count 0.15373304063200652 16\n","45\n","teacher loss  count 0.725031027753002 6\n","non teach loss count 0.1543381700366559 18\n","46\n","teacher loss  count 0.6652299780724624 5\n","non teach loss count 0.15601103320586426 19\n","47\n","teacher loss  count 0.6777600619513733 10\n","non teach loss count 0.16636978940006225 14\n","48\n","teacher loss  count 0.7157153728161844 7\n","non teach loss count 0.151521056937232 17\n","49\n","teacher loss  count 0.7062777847657076 8\n","non teach loss count 0.16062710989403867 16\n","model saved\n","50\n","teacher loss  count 0.7822206936246252 10\n","non teach loss count 0.14034447551611004 14\n","51\n","teacher loss  count 0.683486790557358 7\n","non teach loss count 0.16033701924082067 17\n","52\n","teacher loss  count 0.7042029654814187 14\n","non teach loss count 0.15369004993216567 10\n","53\n","teacher loss  count 0.7129308012464681 8\n","non teach loss count 0.13717289135985059 16\n","54\n","teacher loss  count 0.7083895247843309 13\n","non teach loss count 0.15134832701590897 11\n","55\n","teacher loss  count 0.6250406057637251 6\n","non teach loss count 0.15632687229542946 18\n","56\n","teacher loss  count 0.7102530387471162 7\n","non teach loss count 0.147900744711287 17\n","57\n","teacher loss  count 0.7341209825895065 7\n","non teach loss count 0.13938235342638922 17\n","58\n","teacher loss  count 0.6257425514495973 6\n","non teach loss count 0.13385247791306162 18\n","59\n","teacher loss  count 0.6599379854189239 5\n","non teach loss count 0.14553102035984558 19\n","model saved\n","60\n","teacher loss  count 0.6725029574896371 12\n","non teach loss count 0.1462894145152007 12\n","61\n","teacher loss  count 0.6808638964989383 7\n","non teach loss count 0.1512172995273547 17\n","62\n","teacher loss  count 0.6902420539092919 13\n","non teach loss count 0.14905095154832715 11\n","63\n","teacher loss  count 0.7297710787726274 7\n","non teach loss count 0.15700602027287736 17\n","64\n","teacher loss  count 0.6492057412636072 8\n","non teach loss count 0.14114602064628967 16\n","65\n","teacher loss  count 0.6897800295588045 9\n","non teach loss count 0.14785174542262883 15\n","66\n","teacher loss  count 0.6506292653949234 6\n","non teach loss count 0.14120865272869385 18\n","67\n","teacher loss  count 0.6864632792864325 11\n","non teach loss count 0.14377080640573733 13\n","68\n","teacher loss  count 0.6227246789160028 9\n","non teach loss count 0.14806371463774373 15\n","69\n","teacher loss  count 0.674843122478732 10\n","non teach loss count 0.1532966718886788 14\n","model saved\n","70\n","teacher loss  count 0.7256147362575199 8\n","non teach loss count 0.13792423127866638 16\n","71\n","teacher loss  count 0.7050374102056908 6\n","non teach loss count 0.15099850895529526 18\n","72\n","teacher loss  count 0.6989962314635162 8\n","non teach loss count 0.14924845783068513 16\n","73\n","teacher loss  count 0.704244410530587 9\n","non teach loss count 0.136670297728332 15\n","74\n","teacher loss  count 0.6904636374052125 7\n","non teach loss count 0.15048810774638283 17\n","75\n","teacher loss  count 0.6937063608461748 16\n","non teach loss count 0.17262330953193308 8\n","76\n","teacher loss  count 0.6597873062807452 9\n","non teach loss count 0.1283791877494424 15\n","77\n","teacher loss  count 0.7151557751071521 9\n","non teach loss count 0.1409035216912637 15\n","78\n","teacher loss  count 0.6765902993571146 11\n","non teach loss count 0.14480649826468864 13\n","79\n","teacher loss  count 0.689521979300826 7\n","non teach loss count 0.15283942629813488 17\n","model saved\n","80\n","teacher loss  count 0.6677243770038508 13\n","non teach loss count 0.1424427964285385 11\n","81\n","teacher loss  count 0.6642269944968593 7\n","non teach loss count 0.14910893951801055 17\n","82\n","teacher loss  count 0.6869988782616671 10\n","non teach loss count 0.14945320667174036 14\n","83\n","teacher loss  count 0.6586435832567084 7\n","non teach loss count 0.13502225125960712 17\n","84\n","teacher loss  count 0.6904435117196951 6\n","non teach loss count 0.1453124908597947 18\n","85\n","teacher loss  count 0.6915482869775963 8\n","non teach loss count 0.15058160543559654 16\n","86\n","teacher loss  count 0.6829815845106635 13\n","non teach loss count 0.13776892558860124 11\n","87\n","teacher loss  count 0.6943745645833616 10\n","non teach loss count 0.1349276163724665 14\n","88\n","teacher loss  count 0.718455213196289 10\n","non teach loss count 0.13802258226710093 14\n","89\n","teacher loss  count 0.6637654743834058 8\n","non teach loss count 0.15378483185991565 16\n","model saved\n","90\n","teacher loss  count 0.6703515799948703 10\n","non teach loss count 0.14153331193900964 14\n","91\n","teacher loss  count 0.6562049805907441 8\n","non teach loss count 0.13882717451302684 16\n","92\n","teacher loss  count 0.6806046558687617 8\n","non teach loss count 0.1427819760727907 16\n","93\n","teacher loss  count 0.6906701187382588 12\n","non teach loss count 0.14381470316808717 12\n","94\n","teacher loss  count 0.6672112292239246 10\n","non teach loss count 0.14430015561878146 14\n","95\n","teacher loss  count 0.6909194894407534 11\n","non teach loss count 0.1344314135889824 13\n","96\n","teacher loss  count 0.7005639641350643 7\n","non teach loss count 0.15377987328958356 17\n","97\n","teacher loss  count 0.6733558906117277 7\n","non teach loss count 0.1437331251342 17\n","98\n","teacher loss  count 0.6927893426664129 7\n","non teach loss count 0.1463360336160886 17\n","99\n","teacher loss  count 0.6510106686513498 10\n","non teach loss count 0.14418539170801772 14\n","model saved\n","100\n","teacher loss  count 0.7189527792972296 8\n","non teach loss count 0.14665984658412895 16\n","101\n","teacher loss  count 0.68710109895095 12\n","non teach loss count 0.1358741996143234 12\n","102\n","teacher loss  count 0.6576226319192224 9\n","non teach loss count 0.13604133424397755 15\n","103\n","teacher loss  count 0.6311481935191148 10\n","non teach loss count 0.14378237435277216 14\n","104\n","teacher loss  count 0.6848386462390279 9\n","non teach loss count 0.142879830584691 15\n","105\n","teacher loss  count 0.6817496732714843 5\n","non teach loss count 0.1484460579247972 19\n","106\n","teacher loss  count 0.6757072405224067 5\n","non teach loss count 0.13894977894439922 19\n","107\n","teacher loss  count 0.7009656244046558 8\n","non teach loss count 0.14455536547248307 16\n","108\n","teacher loss  count 0.6816860269120986 13\n","non teach loss count 0.15040863385120914 11\n","109\n","teacher loss  count 0.70300693424961 6\n","non teach loss count 0.14109350064690732 18\n","model saved\n","110\n","teacher loss  count 0.6506634387621723 6\n","non teach loss count 0.14750032424528517 18\n","111\n","teacher loss  count 0.6454581692279188 8\n","non teach loss count 0.12990448368942933 16\n","112\n","teacher loss  count 0.6804725241504187 5\n","non teach loss count 0.1363020106596617 19\n","113\n","teacher loss  count 0.6750630591149562 9\n","non teach loss count 0.14016620215831394 15\n","114\n","teacher loss  count 0.6499166142910993 10\n","non teach loss count 0.1293920209195472 14\n","115\n","teacher loss  count 0.6928067322659751 5\n","non teach loss count 0.13567728205736637 19\n","116\n","teacher loss  count 0.6799306273362042 10\n","non teach loss count 0.15090117120013513 14\n","117\n","teacher loss  count 0.7474002926377058 7\n","non teach loss count 0.14098661727183281 17\n","118\n","teacher loss  count 0.710079617397154 10\n","non teach loss count 0.1486243746322437 14\n","119\n","teacher loss  count 0.7005526155232806 9\n","non teach loss count 0.13535832832599562 15\n","model saved\n","120\n","teacher loss  count 0.6543114874690601 12\n","non teach loss count 0.16117732072694343 12\n","121\n","teacher loss  count 0.6957870710814046 10\n","non teach loss count 0.13376150581603766 14\n","122\n","teacher loss  count 0.701771416804581 3\n","non teach loss count 0.14816192020000998 21\n","123\n","teacher loss  count 0.6856184467018548 6\n","non teach loss count 0.12616361559559952 18\n","124\n","teacher loss  count 0.7717950696923651 3\n","non teach loss count 0.1404019116883666 21\n","125\n","teacher loss  count 0.6269966729815989 7\n","non teach loss count 0.13506165623299227 17\n","126\n","teacher loss  count 0.6596746312454824 5\n","non teach loss count 0.14458855327365253 19\n","127\n","teacher loss  count 0.6546915767734501 9\n","non teach loss count 0.15640130988143663 15\n","128\n","teacher loss  count 0.7181860692332679 5\n","non teach loss count 0.13780514927525425 19\n","129\n","teacher loss  count 0.6928955512366408 13\n","non teach loss count 0.13182540902759987 11\n","model saved\n","130\n","teacher loss  count 0.6517508180913268 10\n","non teach loss count 0.1265745921692313 14\n","131\n","teacher loss  count 0.6930621937396034 11\n","non teach loss count 0.13573131380128012 13\n","132\n","teacher loss  count 0.712305041725258 12\n","non teach loss count 0.12059857750727386 12\n","133\n","teacher loss  count 0.7005709043130927 9\n","non teach loss count 0.14449578116907755 15\n","134\n","teacher loss  count 0.7334664988902274 6\n","non teach loss count 0.13415139352701755 18\n","135\n","teacher loss  count 0.6612047225993966 11\n","non teach loss count 0.13960444772279273 13\n","136\n","teacher loss  count 0.6732011472953484 5\n","non teach loss count 0.144081316534138 19\n","137\n","teacher loss  count 0.7271054867058095 8\n","non teach loss count 0.14698531219624414 16\n","138\n","teacher loss  count 0.6247959280418344 11\n","non teach loss count 0.1400150483265396 13\n","139\n","teacher loss  count 0.6877039381776586 8\n","non teach loss count 0.14388882893501473 16\n","model saved\n","140\n","teacher loss  count 0.6696158442665422 11\n","non teach loss count 0.1449068159500099 13\n","141\n","teacher loss  count 0.6733680082213639 2\n","non teach loss count 0.14191968672803795 22\n","142\n","teacher loss  count 0.7079015521309064 4\n","non teach loss count 0.12926731392220908 20\n","143\n","teacher loss  count 0.711485757948037 10\n","non teach loss count 0.13260174651664564 14\n","144\n","teacher loss  count 0.6502127468112358 7\n","non teach loss count 0.13191056756708086 17\n","145\n","teacher loss  count 0.7345477571006821 5\n","non teach loss count 0.1334449192785367 19\n","146\n","teacher loss  count 0.6615648913280608 8\n","non teach loss count 0.13081454150179914 16\n","147\n","teacher loss  count 0.7034224447352143 11\n","non teach loss count 0.14148673147916496 13\n","148\n","teacher loss  count 0.6466308052024451 9\n","non teach loss count 0.13644870705540643 15\n","149\n","teacher loss  count 0.6740411923328071 10\n","non teach loss count 0.13527706665382763 14\n","model saved\n","150\n","teacher loss  count 0.7392157080710413 4\n","non teach loss count 0.13036357352914643 20\n","151\n","teacher loss  count 0.7048498653564776 7\n","non teach loss count 0.133585976540429 17\n","152\n","teacher loss  count 0.7324407009496209 6\n","non teach loss count 0.14150786537579174 18\n","153\n","teacher loss  count 0.6802644896687357 5\n","non teach loss count 0.1290179593748243 19\n","154\n","teacher loss  count 0.689608623604759 9\n","non teach loss count 0.12360372055779861 15\n","155\n","teacher loss  count 0.6662183703865172 8\n","non teach loss count 0.12520574527576622 16\n","156\n","teacher loss  count 0.6674327873283318 8\n","non teach loss count 0.13641009573469917 16\n","157\n","teacher loss  count 0.6616638359718519 10\n","non teach loss count 0.147304837750112 14\n","158\n","teacher loss  count 0.6697415983341714 8\n","non teach loss count 0.1303912015659119 16\n","159\n","teacher loss  count 0.6318363719046458 4\n","non teach loss count 0.14071605170259377 20\n","model saved\n","160\n","teacher loss  count 0.6682307711465916 9\n","non teach loss count 0.13757499790986838 15\n","161\n","teacher loss  count 0.7419092329524386 8\n","non teach loss count 0.1353055947324421 16\n","162\n","teacher loss  count 0.68956552842124 6\n","non teach loss count 0.1252422246746125 18\n","163\n","teacher loss  count 0.6420897420452387 8\n","non teach loss count 0.14104793182202785 16\n","164\n","teacher loss  count 0.7040077194477737 5\n","non teach loss count 0.1333315414074274 19\n","165\n","teacher loss  count 0.6905577509798668 10\n","non teach loss count 0.13729800944938433 14\n","166\n","teacher loss  count 0.6282393449818399 4\n","non teach loss count 0.13473998424172612 20\n","167\n","teacher loss  count 0.6490669065091142 8\n","non teach loss count 0.14238393577693192 16\n","168\n","teacher loss  count 0.6506778309925935 11\n","non teach loss count 0.13030696176055592 13\n","169\n","teacher loss  count 0.6859642240773408 9\n","non teach loss count 0.13290274256888157 15\n","model saved\n","170\n","teacher loss  count 0.6972432334101439 7\n","non teach loss count 0.138169557699934 17\n","171\n","teacher loss  count 0.6654160895327685 11\n","non teach loss count 0.11810219877948305 13\n","172\n","teacher loss  count 0.7350178222288886 9\n","non teach loss count 0.14225320641521735 15\n","173\n","teacher loss  count 0.6379891718668498 10\n","non teach loss count 0.13123814845964826 14\n","174\n","teacher loss  count 0.6784916477232459 10\n","non teach loss count 0.14117049396692707 14\n","175\n","teacher loss  count 0.7046377348438642 9\n","non teach loss count 0.12535533595772716 15\n","176\n","teacher loss  count 0.6709861090237396 7\n","non teach loss count 0.13206580370220952 17\n","177\n","teacher loss  count 0.683584080107986 10\n","non teach loss count 0.12042609987867713 14\n","178\n","teacher loss  count 0.6458210982449724 8\n","non teach loss count 0.12044230610567398 16\n","179\n","teacher loss  count 0.6687489794461582 8\n","non teach loss count 0.14244595704998947 16\n","model saved\n","180\n","teacher loss  count 0.6612506378174242 8\n","non teach loss count 0.1423038885849797 16\n","181\n","teacher loss  count 0.6789134720462692 7\n","non teach loss count 0.14983633505318214 17\n","182\n","teacher loss  count 0.6460802489703457 11\n","non teach loss count 0.14578588538033477 13\n","183\n","teacher loss  count 0.6914223212713096 6\n","non teach loss count 0.1265831118023914 18\n","184\n","teacher loss  count 0.6818594497250563 10\n","non teach loss count 0.13841621322808098 14\n","185\n","teacher loss  count 0.7317983864084782 8\n","non teach loss count 0.12552902475136574 16\n","186\n","teacher loss  count 0.6826534492326506 10\n","non teach loss count 0.11755119040259422 14\n","187\n","teacher loss  count 0.6884187012890964 13\n","non teach loss count 0.13707445633945878 11\n","188\n","teacher loss  count 0.7254157153140394 8\n","non teach loss count 0.14051289379096002 16\n","189\n","teacher loss  count 0.6503632394501196 11\n","non teach loss count 0.11881358388450236 13\n","model saved\n","190\n","teacher loss  count 0.6608790875159107 9\n","non teach loss count 0.14028579923843665 15\n","191\n","teacher loss  count 0.6662974514378064 11\n","non teach loss count 0.12798933150177513 13\n","192\n","teacher loss  count 0.6614225155885198 9\n","non teach loss count 0.13428276947673481 15\n","193\n","teacher loss  count 0.6247062818312408 8\n","non teach loss count 0.14460923068614642 16\n","194\n","teacher loss  count 0.767330358847273 8\n","non teach loss count 0.12417507890667573 16\n","195\n","teacher loss  count 0.6639287521214084 9\n","non teach loss count 0.1403278311834234 15\n","196\n","teacher loss  count 0.6484537051060866 10\n","non teach loss count 0.13194879182474092 14\n","197\n","teacher loss  count 0.6810744818361094 14\n","non teach loss count 0.14755789227016364 10\n","198\n","teacher loss  count 0.7152292765790164 4\n","non teach loss count 0.14421555094420432 20\n","199\n","teacher loss  count 0.6864679514206316 10\n","non teach loss count 0.14842098724999514 14\n","model saved\n","200\n","teacher loss  count 0.6860230652938355 8\n","non teach loss count 0.15062818193785135 16\n","201\n","teacher loss  count 0.6642722410188322 7\n","non teach loss count 0.1347286560634318 17\n","202\n","teacher loss  count 0.6873174371291957 10\n","non teach loss count 0.13652660005515965 14\n","203\n","teacher loss  count 0.6873222899273627 6\n","non teach loss count 0.12853262876872829 18\n","204\n","teacher loss  count 0.6968400632476244 9\n","non teach loss count 0.13223791192188605 15\n","205\n","teacher loss  count 0 0\n","non teach loss count 0.13368630177687366 24\n","206\n","teacher loss  count 0.6566333399329591 11\n","non teach loss count 0.12126579381446469 13\n","207\n","teacher loss  count 0.6784257332714718 8\n","non teach loss count 0.13573720271724374 16\n","208\n","teacher loss  count 0.682695276814438 8\n","non teach loss count 0.10912942725864042 16\n","209\n","teacher loss  count 0.6769648337141156 10\n","non teach loss count 0.13547654356265082 14\n","model saved\n","210\n","teacher loss  count 0.703740972802529 4\n","non teach loss count 0.12185740578240804 20\n","211\n","teacher loss  count 0.6860449802073981 5\n","non teach loss count 0.1370021139474576 19\n","212\n","teacher loss  count 0.6580732901984031 9\n","non teach loss count 0.1277401007889341 15\n","213\n","teacher loss  count 0.6697282605602954 13\n","non teach loss count 0.13499983698270698 11\n","214\n","teacher loss  count 0.6436977007048728 10\n","non teach loss count 0.11818915017864828 14\n","215\n","teacher loss  count 0.7119787424616669 9\n","non teach loss count 0.12010677320599884 15\n","216\n","teacher loss  count 0.6484349694538227 8\n","non teach loss count 0.1352332226017267 16\n","217\n","teacher loss  count 0.6821916936064042 10\n","non teach loss count 0.12865263399616245 14\n","218\n","teacher loss  count 0.6814679764876725 3\n","non teach loss count 0.12776991873052568 21\n","219\n","teacher loss  count 0.6802164466880193 12\n","non teach loss count 0.1284478288529932 12\n","model saved\n","220\n","teacher loss  count 0.6587854959683592 11\n","non teach loss count 0.13748167940061626 13\n","221\n","teacher loss  count 0.5925387149276413 3\n","non teach loss count 0.12506111752580276 21\n","222\n","teacher loss  count 0.6295838742265707 9\n","non teach loss count 0.13043899195987407 15\n","223\n","teacher loss  count 0.6472913395020935 8\n","non teach loss count 0.13085447833356034 16\n","224\n","teacher loss  count 0.6532074698379764 5\n","non teach loss count 0.14330305419155187 19\n","225\n","teacher loss  count 0.6867839053105687 11\n","non teach loss count 0.12993632242723618 13\n","226\n","teacher loss  count 0.6298826660719474 5\n","non teach loss count 0.12126974694050188 19\n","227\n","teacher loss  count 0.6647400104544349 8\n","non teach loss count 0.12630696736254157 16\n","228\n","teacher loss  count 0.6403824704599547 4\n","non teach loss count 0.11895834632056368 20\n","229\n","teacher loss  count 0.6809170546098716 7\n","non teach loss count 0.13720434234717305 17\n","model saved\n","230\n","teacher loss  count 0.6957054821678379 9\n","non teach loss count 0.12026408403408555 15\n","231\n","teacher loss  count 0.6998984441509798 6\n","non teach loss count 0.1182008355126214 18\n","232\n","teacher loss  count 0.7433913510779038 9\n","non teach loss count 0.12187853186428857 15\n","233\n","teacher loss  count 0.6644579583330653 11\n","non teach loss count 0.12302175927749447 13\n","234\n","teacher loss  count 0.6848147848332817 10\n","non teach loss count 0.13420395544919375 14\n","235\n","teacher loss  count 0.7161713008456615 14\n","non teach loss count 0.1343835593591094 10\n","236\n","teacher loss  count 0.7066594623684281 12\n","non teach loss count 0.13317007572623504 12\n","237\n","teacher loss  count 0.6925383378779925 6\n","non teach loss count 0.1329946248287436 18\n","238\n","teacher loss  count 0.6843755954129749 10\n","non teach loss count 0.11838405466871767 14\n","239\n","teacher loss  count 0.6967773394655707 10\n","non teach loss count 0.1352460633602994 14\n","model saved\n","240\n","teacher loss  count 0.6464325398371372 12\n","non teach loss count 0.13636011881946036 12\n","241\n","teacher loss  count 0.660691471109936 7\n","non teach loss count 0.12760544156083284 17\n","242\n","teacher loss  count 0.6793798032099491 8\n","non teach loss count 0.12965237628654638 16\n","243\n","teacher loss  count 0.6988726097171447 8\n","non teach loss count 0.12566465348847572 16\n","244\n","teacher loss  count 0.6492696956220879 5\n","non teach loss count 0.1240990968471107 19\n","245\n","teacher loss  count 0.6898660481167302 9\n","non teach loss count 0.12880947911101195 15\n","246\n","teacher loss  count 0.6783819058672715 14\n","non teach loss count 0.13470987181099442 10\n","247\n","teacher loss  count 0.7112793064553019 8\n","non teach loss count 0.13130631672608328 16\n","248\n","teacher loss  count 0.6440904699497955 9\n","non teach loss count 0.12455549993944204 15\n","249\n","teacher loss  count 0.6469684268164274 12\n","non teach loss count 0.1284852548566047 12\n","model saved\n","250\n","teacher loss  count 0.648717100132968 8\n","non teach loss count 0.1313229345608195 16\n","251\n","teacher loss  count 0.7212326115748325 8\n","non teach loss count 0.1327947553113931 16\n","252\n","teacher loss  count 0.656136381754362 11\n","non teach loss count 0.12331700639286772 13\n","253\n","teacher loss  count 0.6588899635969138 11\n","non teach loss count 0.12566964427019464 13\n","254\n","teacher loss  count 0.6425509906523432 5\n","non teach loss count 0.1337894984694508 19\n","255\n","teacher loss  count 0.6618835303617495 4\n","non teach loss count 0.12989675011124396 20\n","256\n","teacher loss  count 0.6882086855629546 10\n","non teach loss count 0.11805754529229383 14\n","257\n","teacher loss  count 0.62757869587442 9\n","non teach loss count 0.13517018432905595 15\n","258\n","teacher loss  count 0.6898372503663773 6\n","non teach loss count 0.11928365062596992 18\n","259\n","teacher loss  count 0.6794700178768546 7\n","non teach loss count 0.13907501958432988 17\n","model saved\n","260\n","teacher loss  count 0.7008792912584457 12\n","non teach loss count 0.12745667487577786 12\n","261\n","teacher loss  count 0.7086550058653025 5\n","non teach loss count 0.12273570296809308 19\n","262\n","teacher loss  count 0.671166134584469 9\n","non teach loss count 0.1332774923481629 15\n","263\n","teacher loss  count 0.647785903538017 11\n","non teach loss count 0.13410502025485982 13\n","264\n","teacher loss  count 0.6420700023667038 9\n","non teach loss count 0.11642885028685643 15\n","265\n","teacher loss  count 0.6780320861174566 6\n","non teach loss count 0.13105645260174453 18\n","266\n","teacher loss  count 0.6505175709699333 8\n","non teach loss count 0.1141097615833175 16\n","267\n","teacher loss  count 0.6244831894937335 7\n","non teach loss count 0.12531607880653317 17\n","268\n","teacher loss  count 0.6961933976660665 7\n","non teach loss count 0.11934264242452022 17\n","269\n","teacher loss  count 0.6806689437051747 11\n","non teach loss count 0.12372723375228732 13\n","model saved\n","270\n","teacher loss  count 0.6618486295015784 6\n","non teach loss count 0.1458464401977365 18\n","271\n","teacher loss  count 0.669816571580171 11\n","non teach loss count 0.12751489712075031 13\n","272\n","teacher loss  count 0.6621611956923339 9\n","non teach loss count 0.1288712194822686 15\n","273\n","teacher loss  count 0.636943514205453 9\n","non teach loss count 0.12984787228213704 15\n","274\n","teacher loss  count 0.654730485967958 15\n","non teach loss count 0.12440750522179794 9\n","275\n","teacher loss  count 0.6850025168168968 8\n","non teach loss count 0.12601342216597658 16\n","276\n","teacher loss  count 0.6525434492879838 8\n","non teach loss count 0.13022375205257403 16\n","277\n","teacher loss  count 0.6277756994881913 10\n","non teach loss count 0.1403214473899115 14\n","278\n","teacher loss  count 0.6150667683229104 8\n","non teach loss count 0.1325789053833576 16\n","279\n","teacher loss  count 0.6601703273797024 8\n","non teach loss count 0.11997985689227421 16\n","model saved\n","280\n","teacher loss  count 0.6665185636803528 7\n","non teach loss count 0.140120922071302 17\n","281\n","teacher loss  count 0.6784446691662173 6\n","non teach loss count 0.1220835851774518 18\n","282\n","teacher loss  count 0.6681119264265439 6\n","non teach loss count 0.13323942347821974 18\n","283\n","teacher loss  count 0.6664433910626162 10\n","non teach loss count 0.12658697124111268 14\n","284\n","teacher loss  count 0.7100534316168166 8\n","non teach loss count 0.13213650686358125 16\n","285\n","teacher loss  count 0.598874415632342 4\n","non teach loss count 0.13567217300973475 20\n","286\n","teacher loss  count 0.618398885157067 2\n","non teach loss count 0.13500599078509629 22\n","287\n","teacher loss  count 0.698039572448289 11\n","non teach loss count 0.1275428526064129 13\n","288\n","teacher loss  count 0.6683795720024495 5\n","non teach loss count 0.12667894756392686 19\n","289\n","teacher loss  count 0.6576360256722812 7\n","non teach loss count 0.11975629549176835 17\n","model saved\n","290\n","teacher loss  count 0.7247167725372454 5\n","non teach loss count 0.12066587440161007 19\n","291\n","teacher loss  count 0.6473240653207757 11\n","non teach loss count 0.12456980701809256 13\n","292\n","teacher loss  count 0.6605906726229878 8\n","non teach loss count 0.12829433440498136 16\n","293\n","teacher loss  count 0.6909215229310772 13\n","non teach loss count 0.12265524302720703 11\n","294\n","teacher loss  count 0.6841106893560425 13\n","non teach loss count 0.13730946314026415 11\n","295\n","teacher loss  count 0.6338247304774556 10\n","non teach loss count 0.1440264253703289 14\n","296\n","teacher loss  count 0.6776610420898803 5\n","non teach loss count 0.129196099242498 19\n","297\n","teacher loss  count 0.6931541859548548 6\n","non teach loss count 0.12341709374951512 18\n","298\n","teacher loss  count 0.6774313171069553 12\n","non teach loss count 0.11680829456112002 12\n","299\n","teacher loss  count 0.7118873170773005 9\n","non teach loss count 0.12128980057923969 15\n","model saved\n","300\n","teacher loss  count 0.674479917502402 7\n","non teach loss count 0.14130302075795165 17\n","301\n","teacher loss  count 0.6630825506552344 13\n","non teach loss count 0.12198587518959937 11\n","302\n","teacher loss  count 0.6630698806846563 6\n","non teach loss count 0.14385749425678923 18\n","303\n","teacher loss  count 0.6372577616749253 4\n","non teach loss count 0.12587031363154472 20\n","304\n","teacher loss  count 0.7189404834113056 8\n","non teach loss count 0.11337284966967026 16\n","305\n","teacher loss  count 0.6872971271769273 10\n","non teach loss count 0.1318622508449767 14\n","306\n","teacher loss  count 0.6607920379485405 7\n","non teach loss count 0.131107792101566 17\n","307\n","teacher loss  count 0.6582144789112541 10\n","non teach loss count 0.12392206314712648 14\n","308\n","teacher loss  count 0.6571280630415907 6\n","non teach loss count 0.13158580753499366 18\n","309\n","teacher loss  count 0.6722798357637009 4\n","non teach loss count 0.129076429756047 20\n","model saved\n","310\n","teacher loss  count 0.6186309911873058 7\n","non teach loss count 0.1412134030168519 17\n","311\n","teacher loss  count 0.6765305629560687 8\n","non teach loss count 0.13101771290497508 16\n","312\n","teacher loss  count 0.6412728743338311 11\n","non teach loss count 0.12660635173644244 13\n","313\n","teacher loss  count 0.678134944502056 11\n","non teach loss count 0.12272964357766597 13\n","314\n","teacher loss  count 0.6684493579444373 8\n","non teach loss count 0.11909322790764941 16\n","315\n","teacher loss  count 0.6812579884796961 15\n","non teach loss count 0.1279870910674786 9\n","316\n","teacher loss  count 0.6531327841353405 8\n","non teach loss count 0.12794547689191452 16\n","317\n","teacher loss  count 0.6618871608000508 11\n","non teach loss count 0.12038691293571624 13\n","318\n","teacher loss  count 0.7048536394632176 6\n","non teach loss count 0.11980890351408907 18\n","319\n","teacher loss  count 0.6679871946876992 10\n","non teach loss count 0.12386120784490391 14\n","model saved\n","320\n","teacher loss  count 0.626448198996043 8\n","non teach loss count 0.14472874635295116 16\n","321\n","teacher loss  count 0.6263049619833032 5\n","non teach loss count 0.11592235744044788 19\n","322\n","teacher loss  count 0.6620369346831007 10\n","non teach loss count 0.13105468005907311 14\n","323\n","teacher loss  count 0.646846422853995 8\n","non teach loss count 0.1302196474742483 16\n","324\n","teacher loss  count 0.6646849707334737 12\n","non teach loss count 0.12577430871305226 12\n","325\n","teacher loss  count 0.669665014217923 11\n","non teach loss count 0.12709997694515 13\n","326\n","teacher loss  count 0.6990518926221436 6\n","non teach loss count 0.12442539613271143 18\n","327\n","teacher loss  count 0.7098292701561002 11\n","non teach loss count 0.12466099873597057 13\n","328\n","teacher loss  count 0.6590950451101233 7\n","non teach loss count 0.11651369678266095 17\n","329\n","teacher loss  count 0.6397945161733926 11\n","non teach loss count 0.14083123249527046 13\n","model saved\n","330\n","teacher loss  count 0.6553178522541098 12\n","non teach loss count 0.11918577996034896 12\n","331\n","teacher loss  count 0.6901710477831581 8\n","non teach loss count 0.1252476434890915 16\n","332\n","teacher loss  count 0.642163390446933 11\n","non teach loss count 0.13337433594955161 13\n","333\n","teacher loss  count 0.6458779819304168 5\n","non teach loss count 0.12616915791501265 19\n","334\n","teacher loss  count 0.6555601675552848 6\n","non teach loss count 0.12185089897013755 18\n","335\n","teacher loss  count 0.7192918872546782 6\n","non teach loss count 0.13111522059597402 18\n","336\n","teacher loss  count 0.6621543964736116 10\n","non teach loss count 0.1320830455699165 14\n","337\n","teacher loss  count 0.6939190572357056 8\n","non teach loss count 0.12346211375458518 16\n","338\n","teacher loss  count 0.7402166639630997 5\n","non teach loss count 0.13532987635133137 19\n","339\n","teacher loss  count 0.6761584161328712 8\n","non teach loss count 0.13173620316841272 16\n","model saved\n","340\n","teacher loss  count 0.6232368933254653 9\n","non teach loss count 0.1318880908275683 15\n","341\n","teacher loss  count 0.7371229218011539 5\n","non teach loss count 0.1368955957541058 19\n","342\n","teacher loss  count 0.6421073508908073 7\n","non teach loss count 0.12231968620014456 17\n","343\n","teacher loss  count 0.6898341705426538 7\n","non teach loss count 0.11788385812098573 17\n","344\n","teacher loss  count 0.6727947422179752 9\n","non teach loss count 0.12152812497414194 15\n","345\n","teacher loss  count 0.6117141430473255 7\n","non teach loss count 0.13290054482283525 17\n","346\n","teacher loss  count 0.683365028446139 9\n","non teach loss count 0.12489501773545472 15\n","347\n","teacher loss  count 0.6339975731083616 6\n","non teach loss count 0.12571900262329405 18\n","348\n","teacher loss  count 0.6419579593737347 12\n","non teach loss count 0.12402639217773448 12\n","349\n","teacher loss  count 0.6501977557766196 10\n","non teach loss count 0.11405351566545671 14\n","model saved\n","350\n","teacher loss  count 0.6584782658165332 12\n","non teach loss count 0.11421164509585198 12\n","351\n","teacher loss  count 0.6761279219022271 9\n","non teach loss count 0.12563003046699542 15\n","352\n","teacher loss  count 0.6861692644482428 12\n","non teach loss count 0.12383403698710545 12\n","353\n","teacher loss  count 0.6717545717281163 11\n","non teach loss count 0.12573063107874236 13\n","354\n","teacher loss  count 0.6442000168625179 10\n","non teach loss count 0.11755233231265054 14\n","355\n","teacher loss  count 0.6320889492981978 9\n","non teach loss count 0.12274445846109079 15\n","356\n","teacher loss  count 0.6406168079461256 9\n","non teach loss count 0.1339420793076728 15\n","357\n","teacher loss  count 0.6210375747382109 6\n","non teach loss count 0.131222188923631 18\n","358\n","teacher loss  count 0.6816899460937612 7\n","non teach loss count 0.1296043390776998 17\n","359\n","teacher loss  count 0.7073723146483295 8\n","non teach loss count 0.1281596957729806 16\n","model saved\n","360\n","teacher loss  count 0.6568220175550331 10\n","non teach loss count 0.11602578587230127 14\n","361\n","teacher loss  count 0.7116294136522922 7\n","non teach loss count 0.11726163929380135 17\n","362\n","teacher loss  count 0.6562069991127899 8\n","non teach loss count 0.12557544922881006 16\n","363\n","teacher loss  count 0.6247844638764004 7\n","non teach loss count 0.13040341751289966 17\n","364\n","teacher loss  count 0.6740524724231647 11\n","non teach loss count 0.12121880396833024 13\n","365\n","teacher loss  count 0.6481578350833883 11\n","non teach loss count 0.12339286013446019 13\n","366\n","teacher loss  count 0.6616070611579654 13\n","non teach loss count 0.11335944840122475 11\n","367\n","teacher loss  count 0.673838326834863 11\n","non teach loss count 0.11631337733524796 13\n","368\n","teacher loss  count 0.6730728115485134 10\n","non teach loss count 0.11973404160202904 14\n","369\n","teacher loss  count 0.6702513500312435 8\n","non teach loss count 0.11503781450224873 16\n","model saved\n","370\n","teacher loss  count 0.6890393813738596 4\n","non teach loss count 0.12127437316208127 20\n","371\n","teacher loss  count 0.7320419480944658 8\n","non teach loss count 0.12703719771938687 16\n","372\n","teacher loss  count 0.687773951377078 5\n","non teach loss count 0.13126604837775205 19\n","373\n","teacher loss  count 0.6312965239400872 8\n","non teach loss count 0.11667137582031017 16\n","374\n","teacher loss  count 0.6867999245055734 7\n","non teach loss count 0.13521612897298207 17\n","375\n","teacher loss  count 0.7088935859902246 8\n","non teach loss count 0.12595919733807942 16\n","376\n","teacher loss  count 0.6592048268836623 11\n","non teach loss count 0.11447659767004252 13\n","377\n","teacher loss  count 0.6731297636687394 10\n","non teach loss count 0.1293369515896622 14\n","378\n","teacher loss  count 0.6737371556995453 8\n","non teach loss count 0.1322546796326847 16\n","379\n","teacher loss  count 0.6733036400266177 16\n","non teach loss count 0.1230893046727286 8\n","model saved\n","380\n","teacher loss  count 0.7177088742366496 7\n","non teach loss count 0.1256576681271892 17\n","381\n","teacher loss  count 0.6514714033033432 8\n","non teach loss count 0.1203628459803813 16\n","382\n","teacher loss  count 0.6689224617203062 5\n","non teach loss count 0.12545711613096247 19\n","383\n","teacher loss  count 0.6254348733881356 7\n","non teach loss count 0.11508021885779879 17\n","384\n","teacher loss  count 0.6272035533404792 8\n","non teach loss count 0.13476029250013902 16\n","385\n","teacher loss  count 0.649821988290915 14\n","non teach loss count 0.1159143233462602 10\n","386\n","teacher loss  count 0.6257550291922095 13\n","non teach loss count 0.12613933197462435 11\n","387\n","teacher loss  count 0.6362465688536084 7\n","non teach loss count 0.11586584801141606 17\n","388\n","teacher loss  count 0.6724297990069384 6\n","non teach loss count 0.11954791583561751 18\n","389\n","teacher loss  count 0.643208350522368 7\n","non teach loss count 0.11566226298295615 17\n","model saved\n","390\n","teacher loss  count 0.6909577417085679 9\n","non teach loss count 0.13352659145216753 15\n","391\n","teacher loss  count 0.6432422478639243 8\n","non teach loss count 0.12308879134399262 16\n","392\n","teacher loss  count 0.7039533939300223 11\n","non teach loss count 0.14948905960500988 13\n","393\n","teacher loss  count 0.6650807684854146 7\n","non teach loss count 0.12555797673016397 17\n","394\n","teacher loss  count 0.6776662983605102 7\n","non teach loss count 0.12714678452452335 17\n","395\n","teacher loss  count 0.6317852545512093 9\n","non teach loss count 0.14421416560325845 15\n","396\n","teacher loss  count 0.6868023849554257 8\n","non teach loss count 0.11418009538041653 16\n","397\n","teacher loss  count 0.6630715296239809 10\n","non teach loss count 0.11941578211627746 14\n","398\n","teacher loss  count 0.6621619027630369 10\n","non teach loss count 0.12453585278458276 14\n","399\n","teacher loss  count 0.6534529643013061 11\n","non teach loss count 0.1136864432947555 13\n","model saved\n","400\n","teacher loss  count 0.7139879604918375 8\n","non teach loss count 0.1277501138250192 16\n","401\n","teacher loss  count 0.6411421734251783 5\n","non teach loss count 0.12096004161354937 19\n","402\n","teacher loss  count 0.6511046077086547 8\n","non teach loss count 0.11353794236697975 16\n","403\n","teacher loss  count 0.6632604716826936 7\n","non teach loss count 0.13276906453672427 17\n","404\n","teacher loss  count 0.6921067733819111 10\n","non teach loss count 0.11788415440570246 14\n","405\n","teacher loss  count 0.6495353272906396 9\n","non teach loss count 0.12364910286045244 15\n","406\n","teacher loss  count 0.693591153877965 7\n","non teach loss count 0.11675414190525944 17\n","407\n","teacher loss  count 0.6553152362425931 8\n","non teach loss count 0.12084371114546108 16\n","408\n","teacher loss  count 0.6518597844688419 6\n","non teach loss count 0.11253200879629 18\n","409\n","teacher loss  count 0.6633103470390818 9\n","non teach loss count 0.12070537032130457 15\n","model saved\n","410\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9PQXT6c3FHp9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}